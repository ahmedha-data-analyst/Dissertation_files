{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66881b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= All-in-one: models + trainer (fixed for robust GAT/GATv2) =========\n",
    "import os, numpy as np, pandas as pd, torch\n",
    "from torch import nn\n",
    "from torch_geometric.datasets import EllipticBitcoinDataset\n",
    "from torch_geometric.utils import to_undirected, add_self_loops\n",
    "from torch_geometric.nn import GATConv, GATv2Conv, GCNConv, SAGEConv\n",
    "\n",
    "EMB_DIM = 64  # standardized embedding size\n",
    "\n",
    "# ---------- Base ----------\n",
    "class BaseGNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._last_z = None  # cache embeddings\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z, logits = self._embed_and_logits(x, edge_index)\n",
    "        self._last_z = z\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed(self, x, edge_index):\n",
    "        self.eval()\n",
    "        z, _ = self._embed_and_logits(x, edge_index)\n",
    "        return z\n",
    "\n",
    "\n",
    "# ---------- GCN ----------\n",
    "class GCNNet(BaseGNN):\n",
    "    def __init__(self, in_dim, hidden=128, drop=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden, cached=True, normalize=True)\n",
    "        self.bn1   = nn.BatchNorm1d(hidden)\n",
    "        self.drop  = nn.Dropout(drop)\n",
    "        self.act   = nn.ReLU()\n",
    "        self.conv2 = GCNConv(hidden, 2, cached=True, normalize=True)  # logits\n",
    "        self.emb_proj = nn.Linear(hidden, EMB_DIM)\n",
    "        self.emb_bn   = nn.BatchNorm1d(EMB_DIM)\n",
    "\n",
    "    def _embed_and_logits(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = self.act(self.bn1(h))\n",
    "        h = self.drop(h)\n",
    "        logits = self.conv2(h, edge_index)\n",
    "        z = self.emb_bn(self.emb_proj(h))\n",
    "        return z, logits\n",
    "\n",
    "\n",
    "# ---------- Skip-GCN ----------\n",
    "class SkipGCNNet(BaseGNN):\n",
    "    def __init__(self, in_dim, hidden=128, drop=0.5):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(in_dim, EMB_DIM, bias=False)\n",
    "        self.g1 = GCNConv(in_dim, hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden)\n",
    "        self.g2 = GCNConv(hidden, EMB_DIM)\n",
    "        self.bn2 = nn.BatchNorm1d(EMB_DIM)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.act = nn.ReLU()\n",
    "        self.head = nn.Linear(EMB_DIM, 2)\n",
    "\n",
    "    def _embed_and_logits(self, x, edge_index):\n",
    "        skip = self.in_proj(x)\n",
    "        h = self.g1(x, edge_index); h = self.act(self.bn1(h)); h = self.drop(h)\n",
    "        h = self.g2(h, edge_index); h = self.bn2(h)\n",
    "        z = self.act(h + skip); z = self.drop(z)\n",
    "        logits = self.head(z)\n",
    "        return z, logits\n",
    "\n",
    "\n",
    "# ---------- GraphSAGE ----------\n",
    "class SAGENet(BaseGNN):\n",
    "    def __init__(self, in_dim, hidden=256, drop=0.5):\n",
    "        super().__init__()\n",
    "        self.s1 = SAGEConv(in_dim, hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden)\n",
    "        self.s2 = SAGEConv(hidden, EMB_DIM)\n",
    "        self.bn2 = nn.BatchNorm1d(EMB_DIM)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.act = nn.ReLU()\n",
    "        self.head = nn.Linear(EMB_DIM, 2)\n",
    "\n",
    "    def _embed_and_logits(self, x, edge_index):\n",
    "        h = self.s1(x, edge_index); h = self.act(self.bn1(h)); h = self.drop(h)\n",
    "        z = self.s2(h, edge_index); z = self.act(self.bn2(z)); z = self.drop(z)\n",
    "        logits = self.head(z)\n",
    "        return z, logits\n",
    "\n",
    "\n",
    "# ---------- Classic GAT (direct logits, LayerNorm, no double self-loops) ----------\n",
    "class GATNet(BaseGNN):\n",
    "    def __init__(self, in_dim, hidden=128, heads=4, drop=0.5):\n",
    "        super().__init__()\n",
    "        self.g1 = GATConv(in_dim, hidden, heads=heads, concat=True,\n",
    "                          dropout=drop, add_self_loops=False)\n",
    "        self.n1 = nn.LayerNorm(hidden * heads)\n",
    "        self.act = nn.ELU()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        # direct logits head (no EMB projection in the classification path)\n",
    "        self.g2 = GATConv(hidden * heads, 2, heads=1, concat=False,\n",
    "                          dropout=drop, add_self_loops=False)\n",
    "        # separate embedding projection from the penultimate representation\n",
    "        self.emb = nn.Sequential(\n",
    "            nn.Linear(hidden * heads, EMB_DIM, bias=False),\n",
    "            nn.LayerNorm(EMB_DIM)\n",
    "        )\n",
    "\n",
    "    def _embed_and_logits(self, x, edge_index):\n",
    "        h1 = self.g1(x, edge_index); h1 = self.act(self.n1(h1)); h1 = self.drop(h1)\n",
    "        logits = self.g2(h1, edge_index)\n",
    "        z = self.emb(h1)\n",
    "        return z, logits\n",
    "\n",
    "\n",
    "# ---------- GATv2 (direct logits, LayerNorm, no double self-loops) ----------\n",
    "class GATv2Net(BaseGNN):\n",
    "    def __init__(self, in_dim, hidden=128, heads=4, drop=0.5):\n",
    "        super().__init__()\n",
    "        self.g1 = GATv2Conv(in_dim, hidden, heads=heads, concat=True,\n",
    "                            dropout=drop, add_self_loops=False)\n",
    "        self.n1 = nn.LayerNorm(hidden * heads)\n",
    "        self.act = nn.ELU()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.g2 = GATv2Conv(hidden * heads, 2, heads=1, concat=False,\n",
    "                            dropout=drop, add_self_loops=False)  # direct logits\n",
    "        self.emb = nn.Sequential(\n",
    "            nn.Linear(hidden * heads, EMB_DIM, bias=False),\n",
    "            nn.LayerNorm(EMB_DIM)\n",
    "        )\n",
    "\n",
    "    def _embed_and_logits(self, x, edge_index):\n",
    "        h1 = self.g1(x, edge_index); h1 = self.act(self.n1(h1)); h1 = self.drop(h1)\n",
    "        logits = self.g2(h1, edge_index)\n",
    "        z = self.emb(h1)\n",
    "        return z, logits\n",
    "\n",
    "\n",
    "# ---------- EvolveGCN (guarded) ----------\n",
    "try:\n",
    "    from torch_geometric_temporal.nn.recurrent import EvolveGCNO\n",
    "    HAVE_TGT = True\n",
    "except Exception:\n",
    "    HAVE_TGT = False\n",
    "\n",
    "class EvolveGCNWrapper(BaseGNN):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        if not HAVE_TGT:\n",
    "            raise RuntimeError(\"EvolveGCN requires torch_geometric_temporal + temporal snapshots.\")\n",
    "        self.cell = EvolveGCNO(in_dim, EMB_DIM)\n",
    "        self.head = nn.Linear(EMB_DIM, 2)\n",
    "\n",
    "    def _embed_and_logits(self, x, edge_index):\n",
    "        raise RuntimeError(\"Provide temporal snapshots iterator for EvolveGCN.\")\n",
    "\n",
    "\n",
    "# ---------- Trainer ----------\n",
    "def gnn_train(\n",
    "    arch=\"gcn\",\n",
    "    root=\"./elliptic_data\",\n",
    "    seed=42,\n",
    "    epochs=1000,\n",
    "    patience=50,\n",
    "    lr=None,\n",
    "    wd=None,\n",
    "    device=None,\n",
    "    save_to_disk=False,\n",
    "    out_dir=\"embeddings\",\n",
    "    use_rolled_val=True,   # use 31–34 for threshold tuning\n",
    "    grad_clip=None         # None disables; set e.g. 2.0 for GCN/SAGE\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a GNN on Elliptic with temporal splits and save 64-d embeddings.\n",
    "\n",
    "    Usage:\n",
    "        results = gnn_train(\"gatv2\", epochs=500)\n",
    "        results = gnn_train(\"gcn\", lr=0.01)\n",
    "        results = gnn_train(\"gat\", grad_clip=None)\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed); np.random.seed(seed)\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Data: temporal split + preprocess\n",
    "    dataset = EllipticBitcoinDataset(root=root)\n",
    "    data = dataset[0]\n",
    "    feat_df = pd.read_csv(os.path.join(root, \"raw\", \"elliptic_txs_features.csv\"), header=None)\n",
    "    time_step_cpu = torch.from_numpy(feat_df[1].values)  # CPU tensor for masks\n",
    "    labeled_cpu = (data.y != 2).cpu()\n",
    "\n",
    "    # Wider validation window for robust threshold transfer\n",
    "    # val: 31–34, train: <31, test: >=35\n",
    "    val_mask_cpu   = (time_step_cpu >= 31) & (time_step_cpu < 35) & labeled_cpu if use_rolled_val \\\n",
    "                     else ((time_step_cpu >= 32) & (time_step_cpu < 35) & labeled_cpu)\n",
    "    train_mask_cpu = (time_step_cpu < (31 if use_rolled_val else 35)) & labeled_cpu & (~val_mask_cpu)\n",
    "    test_mask_cpu  = (time_step_cpu >= 35) & labeled_cpu\n",
    "\n",
    "    # Graph + self-loops (added once globally)\n",
    "    data.edge_index = to_undirected(data.edge_index, num_nodes=data.num_nodes)\n",
    "    data.edge_index, _ = add_self_loops(data.edge_index, num_nodes=data.num_nodes)\n",
    "\n",
    "    # Train-only z-score\n",
    "    with torch.no_grad():\n",
    "        tr = train_mask_cpu\n",
    "        mu = data.x[tr].mean(0, keepdim=True)\n",
    "        std = data.x[tr].std(0, keepdim=True).clamp_min(1e-6)\n",
    "        data.x = ((data.x - mu) / std).to(torch.float)\n",
    "\n",
    "    # Send to device\n",
    "    data = data.to(device); data.y = data.y.long().to(device)\n",
    "    data.train_mask = train_mask_cpu.to(device)\n",
    "    data.val_mask   = val_mask_cpu.to(device)\n",
    "    data.test_mask  = test_mask_cpu.to(device)\n",
    "\n",
    "    # Model factory + defaults\n",
    "    arch_lc = arch.lower()\n",
    "    defaults = {\n",
    "        \"gatv2\": dict(lr=3e-3, wd=5e-4, grad_clip=None),\n",
    "        \"gat\":   dict(lr=3e-3, wd=5e-4, grad_clip=None),\n",
    "        \"gcn\":   dict(lr=1e-2, wd=5e-4, grad_clip=2.0),\n",
    "        \"skip_gcn\": dict(lr=1e-2, wd=5e-4, grad_clip=2.0),\n",
    "        \"sage\":  dict(lr=1e-2, wd=5e-4, grad_clip=2.0),\n",
    "        \"evolvegcn\": dict(lr=5e-3, wd=5e-4, grad_clip=None),\n",
    "    }\n",
    "    if arch_lc not in defaults:\n",
    "        raise ValueError(\"arch must be one of: gcn, skip_gcn, gat, gatv2, sage, evolvegcn\")\n",
    "\n",
    "    factory = {\n",
    "        \"gatv2\":     lambda: GATv2Net(in_dim=data.num_features, hidden=128, heads=4, drop=0.5),\n",
    "        \"gat\":       lambda: GATNet(in_dim=data.num_features,  hidden=128, heads=4, drop=0.5),\n",
    "        \"gcn\":       lambda: GCNNet(in_dim=data.num_features,  hidden=128, drop=0.5),\n",
    "        \"skip_gcn\":  lambda: SkipGCNNet(in_dim=data.num_features, hidden=128, drop=0.5),\n",
    "        \"sage\":      lambda: SAGENet(in_dim=data.num_features,  hidden=256, drop=0.5),\n",
    "        \"evolvegcn\": lambda: EvolveGCNWrapper(in_dim=data.num_features),\n",
    "    }\n",
    "    model = factory[arch_lc]().to(device)\n",
    "    lr = defaults[arch_lc][\"lr\"] if lr is None else lr\n",
    "    wd = defaults[arch_lc][\"wd\"] if wd is None else wd\n",
    "    if grad_clip is None and defaults[arch_lc][\"grad_clip\"] is not None:\n",
    "        grad_clip = defaults[arch_lc][\"grad_clip\"]\n",
    "\n",
    "    # Loss/optim (class imbalance)\n",
    "    y_tr = data.y[data.train_mask]\n",
    "    pos = int((y_tr == 1).sum())\n",
    "    neg = int((y_tr == 0).sum())\n",
    "    weights = torch.tensor([1.0, neg / max(1, pos)], dtype=torch.float, device=device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    # Helpers\n",
    "    @torch.no_grad()\n",
    "    def best_threshold(logits, y_true):\n",
    "        p1 = logits.softmax(1)[:, 1]\n",
    "        best_f1, best_t = -1.0, 0.5\n",
    "        for t in torch.linspace(0.05, 0.95, steps=37, device=p1.device):\n",
    "            yhat = (p1 >= t).long()\n",
    "            tp = ((yhat==1)&(y_true==1)).sum().item()\n",
    "            fp = ((yhat==1)&(y_true==0)).sum().item()\n",
    "            fn = ((yhat==0)&(y_true==1)).sum().item()\n",
    "            P = 0.0 if tp+fp==0 else tp/(tp+fp)\n",
    "            R = 0.0 if tp+fn==0 else tp/(tp+fn)\n",
    "            F1 = 0.0 if P+R==0 else 2*P*R/(P+R)\n",
    "            if F1 > best_f1:\n",
    "                best_f1, best_t = F1, float(t.item())\n",
    "        return best_t, best_f1\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def report(split_mask, thr):\n",
    "        logits = model(data.x, data.edge_index)[split_mask]\n",
    "        y = data.y[split_mask]\n",
    "        p1 = logits.softmax(1)[:, 1]\n",
    "        yhat = (p1 >= thr).long()\n",
    "        tp = ((yhat==1)&(y==1)).sum().item()\n",
    "        fp = ((yhat==1)&(y==0)).sum().item()\n",
    "        fn = ((yhat==0)&(y==1)).sum().item()\n",
    "        tn = ((yhat==0)&(y==0)).sum().item()\n",
    "        P = 0.0 if tp+fp==0 else tp/(tp+fp)\n",
    "        R = 0.0 if tp+fn==0 else tp/(tp+fn)\n",
    "        F1 = 0.0 if P+R==0 else 2*P*R/(P+R)\n",
    "        Acc = (tp+tn)/max(1,tp+tn+fp+fn)\n",
    "        return dict(P=P, R=R, F1=F1, Acc=Acc, microF1=Acc)\n",
    "\n",
    "    # Train (early stop on val F1)\n",
    "    best = {\"val_f1\": -1.0, \"thr\": 0.5, \"state\": None}\n",
    "    waited = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train(); opt.zero_grad()\n",
    "        logits_all = model(data.x, data.edge_index)\n",
    "        loss = criterion(logits_all[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        if grad_clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        opt.step()\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            model.eval()\n",
    "            val_logits = model(data.x, data.edge_index)[data.val_mask]\n",
    "            thr, f1 = best_threshold(val_logits, data.y[data.val_mask])\n",
    "            if f1 > best[\"val_f1\"]:\n",
    "                best = {\n",
    "                    \"val_f1\": f1,\n",
    "                    \"thr\": thr,\n",
    "                    \"state\": {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "                }\n",
    "                waited = 0\n",
    "            else:\n",
    "                waited += 5\n",
    "            print(f\"[{arch.upper():6}] Epoch {epoch:04d} | Loss {loss.item():.4f} | Val F1(illicit) {f1:.3f} | Thr {thr:.2f}\")\n",
    "            if waited >= patience:\n",
    "                break\n",
    "\n",
    "    # Test + embeddings\n",
    "    model.load_state_dict(best[\"state\"]); model.to(device); model.eval()\n",
    "    test = report(data.test_mask, thr=best[\"thr\"])\n",
    "    with torch.no_grad():\n",
    "        z = model.embed(data.x, data.edge_index).detach().cpu().numpy()\n",
    "\n",
    "    var_name = f\"{arch_lc}_embeddings\"\n",
    "    globals()[var_name] = z\n",
    "    nice = arch.replace(\"_\",\" \").title().replace(\"Gcn\",\"GCN\").replace(\"Gat\",\"GAT\")\n",
    "    print(\"Embeddings ready:\"); print(f\"{nice}: {z.shape}\")\n",
    "\n",
    "    if save_to_disk:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        np.save(os.path.join(out_dir, f\"{arch_lc}_embeddings.npy\"), z)\n",
    "\n",
    "    print(f\"\\n[{arch.upper()}] TEST → F1(illicit): {test['F1']:.3f}, Precision: {test['P']:.3f}, \"\n",
    "          f\"Recall: {test['R']:.3f}, Acc: {test['Acc']:.3f}, Micro-F1: {test['microF1']:.3f}\")\n",
    "\n",
    "    print(\"TIP: Call gnn_train('arch_name') where arch_name ∈ [gcn, skip_gcn, gat, gatv2, sage, evolvegcn]\")\n",
    "\n",
    "    return {\n",
    "        \"arch\": arch_lc,\n",
    "        \"val_best_F1\": best[\"val_f1\"],\n",
    "        \"best_threshold\": best[\"thr\"],\n",
    "        \"test_metrics\": test,\n",
    "        \"embeddings_var\": var_name,\n",
    "        \"embeddings_shape\": z.shape,\n",
    "        \"lr\": lr,\n",
    "        \"weight_decay\": wd,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f4a9473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training GCN ===\n",
      "[GCN   ] Epoch 0001 | Loss 0.6614 | Val F1(illicit) 0.428 | Thr 0.88\n",
      "[GCN   ] Epoch 0005 | Loss 0.3461 | Val F1(illicit) 0.417 | Thr 0.95\n",
      "[GCN   ] Epoch 0010 | Loss 0.2892 | Val F1(illicit) 0.522 | Thr 0.85\n",
      "[GCN   ] Epoch 0015 | Loss 0.2556 | Val F1(illicit) 0.629 | Thr 0.90\n",
      "[GCN   ] Epoch 0020 | Loss 0.2280 | Val F1(illicit) 0.689 | Thr 0.88\n",
      "[GCN   ] Epoch 0025 | Loss 0.2106 | Val F1(illicit) 0.718 | Thr 0.82\n",
      "[GCN   ] Epoch 0030 | Loss 0.1947 | Val F1(illicit) 0.713 | Thr 0.77\n",
      "[GCN   ] Epoch 0035 | Loss 0.1837 | Val F1(illicit) 0.740 | Thr 0.70\n",
      "[GCN   ] Epoch 0040 | Loss 0.1730 | Val F1(illicit) 0.746 | Thr 0.68\n",
      "[GCN   ] Epoch 0045 | Loss 0.1635 | Val F1(illicit) 0.773 | Thr 0.62\n",
      "[GCN   ] Epoch 0050 | Loss 0.1552 | Val F1(illicit) 0.801 | Thr 0.57\n",
      "[GCN   ] Epoch 0055 | Loss 0.1455 | Val F1(illicit) 0.801 | Thr 0.62\n",
      "[GCN   ] Epoch 0060 | Loss 0.1446 | Val F1(illicit) 0.801 | Thr 0.60\n",
      "[GCN   ] Epoch 0065 | Loss 0.1368 | Val F1(illicit) 0.797 | Thr 0.62\n",
      "[GCN   ] Epoch 0070 | Loss 0.1247 | Val F1(illicit) 0.809 | Thr 0.53\n",
      "[GCN   ] Epoch 0075 | Loss 0.1266 | Val F1(illicit) 0.815 | Thr 0.57\n",
      "[GCN   ] Epoch 0080 | Loss 0.1253 | Val F1(illicit) 0.813 | Thr 0.65\n",
      "[GCN   ] Epoch 0085 | Loss 0.1214 | Val F1(illicit) 0.826 | Thr 0.73\n",
      "[GCN   ] Epoch 0090 | Loss 0.1215 | Val F1(illicit) 0.812 | Thr 0.57\n",
      "[GCN   ] Epoch 0095 | Loss 0.1188 | Val F1(illicit) 0.801 | Thr 0.77\n",
      "[GCN   ] Epoch 0100 | Loss 0.1198 | Val F1(illicit) 0.817 | Thr 0.73\n",
      "[GCN   ] Epoch 0105 | Loss 0.1212 | Val F1(illicit) 0.820 | Thr 0.80\n",
      "[GCN   ] Epoch 0110 | Loss 0.1180 | Val F1(illicit) 0.808 | Thr 0.82\n",
      "[GCN   ] Epoch 0115 | Loss 0.1155 | Val F1(illicit) 0.810 | Thr 0.77\n",
      "[GCN   ] Epoch 0120 | Loss 0.1127 | Val F1(illicit) 0.816 | Thr 0.77\n",
      "[GCN   ] Epoch 0125 | Loss 0.1087 | Val F1(illicit) 0.822 | Thr 0.80\n",
      "[GCN   ] Epoch 0130 | Loss 0.1151 | Val F1(illicit) 0.815 | Thr 0.80\n",
      "[GCN   ] Epoch 0135 | Loss 0.1070 | Val F1(illicit) 0.818 | Thr 0.65\n",
      "Embeddings ready:\n",
      "GCN: (203769, 64)\n",
      "\n",
      "[GCN] TEST → F1(illicit): 0.535, Precision: 0.486, Recall: 0.596, Acc: 0.933, Micro-F1: 0.933\n",
      "TIP: Call gnn_train('arch_name') where arch_name ∈ [gcn, skip_gcn, gat, gatv2, sage, evolvegcn]\n",
      "\n",
      "=== Training SKIP_GCN ===\n",
      "[SKIP_GCN] Epoch 0001 | Loss 0.6942 | Val F1(illicit) 0.477 | Thr 0.73\n",
      "[SKIP_GCN] Epoch 0005 | Loss 0.3254 | Val F1(illicit) 0.614 | Thr 0.90\n",
      "[SKIP_GCN] Epoch 0010 | Loss 0.2415 | Val F1(illicit) 0.623 | Thr 0.95\n",
      "[SKIP_GCN] Epoch 0015 | Loss 0.1985 | Val F1(illicit) 0.626 | Thr 0.93\n",
      "[SKIP_GCN] Epoch 0020 | Loss 0.1698 | Val F1(illicit) 0.660 | Thr 0.85\n",
      "[SKIP_GCN] Epoch 0025 | Loss 0.1541 | Val F1(illicit) 0.683 | Thr 0.82\n",
      "[SKIP_GCN] Epoch 0030 | Loss 0.1350 | Val F1(illicit) 0.705 | Thr 0.73\n",
      "[SKIP_GCN] Epoch 0035 | Loss 0.1246 | Val F1(illicit) 0.717 | Thr 0.53\n",
      "[SKIP_GCN] Epoch 0040 | Loss 0.1135 | Val F1(illicit) 0.725 | Thr 0.35\n",
      "[SKIP_GCN] Epoch 0045 | Loss 0.1005 | Val F1(illicit) 0.752 | Thr 0.32\n",
      "[SKIP_GCN] Epoch 0050 | Loss 0.1008 | Val F1(illicit) 0.765 | Thr 0.22\n",
      "[SKIP_GCN] Epoch 0055 | Loss 0.0937 | Val F1(illicit) 0.795 | Thr 0.50\n",
      "[SKIP_GCN] Epoch 0060 | Loss 0.0875 | Val F1(illicit) 0.812 | Thr 0.57\n",
      "[SKIP_GCN] Epoch 0065 | Loss 0.0814 | Val F1(illicit) 0.820 | Thr 0.57\n",
      "[SKIP_GCN] Epoch 0070 | Loss 0.0765 | Val F1(illicit) 0.831 | Thr 0.47\n",
      "[SKIP_GCN] Epoch 0075 | Loss 0.0794 | Val F1(illicit) 0.819 | Thr 0.42\n",
      "[SKIP_GCN] Epoch 0080 | Loss 0.0838 | Val F1(illicit) 0.850 | Thr 0.75\n",
      "[SKIP_GCN] Epoch 0085 | Loss 0.0742 | Val F1(illicit) 0.838 | Thr 0.68\n",
      "[SKIP_GCN] Epoch 0090 | Loss 0.0672 | Val F1(illicit) 0.839 | Thr 0.57\n",
      "[SKIP_GCN] Epoch 0095 | Loss 0.0651 | Val F1(illicit) 0.830 | Thr 0.40\n",
      "[SKIP_GCN] Epoch 0100 | Loss 0.0621 | Val F1(illicit) 0.845 | Thr 0.45\n",
      "[SKIP_GCN] Epoch 0105 | Loss 0.0655 | Val F1(illicit) 0.859 | Thr 0.68\n",
      "[SKIP_GCN] Epoch 0110 | Loss 0.0603 | Val F1(illicit) 0.836 | Thr 0.60\n",
      "[SKIP_GCN] Epoch 0115 | Loss 0.0587 | Val F1(illicit) 0.852 | Thr 0.50\n",
      "[SKIP_GCN] Epoch 0120 | Loss 0.0530 | Val F1(illicit) 0.864 | Thr 0.65\n",
      "[SKIP_GCN] Epoch 0125 | Loss 0.0578 | Val F1(illicit) 0.858 | Thr 0.60\n",
      "[SKIP_GCN] Epoch 0130 | Loss 0.0543 | Val F1(illicit) 0.847 | Thr 0.55\n",
      "[SKIP_GCN] Epoch 0135 | Loss 0.0554 | Val F1(illicit) 0.835 | Thr 0.53\n",
      "[SKIP_GCN] Epoch 0140 | Loss 0.0513 | Val F1(illicit) 0.856 | Thr 0.70\n",
      "[SKIP_GCN] Epoch 0145 | Loss 0.0487 | Val F1(illicit) 0.850 | Thr 0.27\n",
      "[SKIP_GCN] Epoch 0150 | Loss 0.0564 | Val F1(illicit) 0.865 | Thr 0.47\n",
      "[SKIP_GCN] Epoch 0155 | Loss 0.0547 | Val F1(illicit) 0.846 | Thr 0.47\n",
      "[SKIP_GCN] Epoch 0160 | Loss 0.0541 | Val F1(illicit) 0.821 | Thr 0.45\n",
      "[SKIP_GCN] Epoch 0165 | Loss 0.0477 | Val F1(illicit) 0.839 | Thr 0.37\n",
      "[SKIP_GCN] Epoch 0170 | Loss 0.0494 | Val F1(illicit) 0.865 | Thr 0.70\n",
      "[SKIP_GCN] Epoch 0175 | Loss 0.0448 | Val F1(illicit) 0.867 | Thr 0.45\n",
      "[SKIP_GCN] Epoch 0180 | Loss 0.0461 | Val F1(illicit) 0.840 | Thr 0.47\n",
      "[SKIP_GCN] Epoch 0185 | Loss 0.0467 | Val F1(illicit) 0.845 | Thr 0.65\n",
      "[SKIP_GCN] Epoch 0190 | Loss 0.0471 | Val F1(illicit) 0.842 | Thr 0.42\n",
      "[SKIP_GCN] Epoch 0195 | Loss 0.0431 | Val F1(illicit) 0.848 | Thr 0.55\n",
      "[SKIP_GCN] Epoch 0200 | Loss 0.0417 | Val F1(illicit) 0.822 | Thr 0.40\n",
      "[SKIP_GCN] Epoch 0205 | Loss 0.0452 | Val F1(illicit) 0.839 | Thr 0.68\n",
      "[SKIP_GCN] Epoch 0210 | Loss 0.0464 | Val F1(illicit) 0.853 | Thr 0.68\n",
      "[SKIP_GCN] Epoch 0215 | Loss 0.0477 | Val F1(illicit) 0.834 | Thr 0.45\n",
      "[SKIP_GCN] Epoch 0220 | Loss 0.0470 | Val F1(illicit) 0.821 | Thr 0.17\n",
      "[SKIP_GCN] Epoch 0225 | Loss 0.0475 | Val F1(illicit) 0.847 | Thr 0.65\n",
      "Embeddings ready:\n",
      "Skip GCN: (203769, 64)\n",
      "\n",
      "[SKIP_GCN] TEST → F1(illicit): 0.606, Precision: 0.610, Recall: 0.603, Acc: 0.949, Micro-F1: 0.949\n",
      "TIP: Call gnn_train('arch_name') where arch_name ∈ [gcn, skip_gcn, gat, gatv2, sage, evolvegcn]\n",
      "\n",
      "=== Training GAT ===\n",
      "[GAT   ] Epoch 0001 | Loss 1.2706 | Val F1(illicit) 0.378 | Thr 0.95\n",
      "[GAT   ] Epoch 0005 | Loss 0.9281 | Val F1(illicit) 0.466 | Thr 0.95\n",
      "[GAT   ] Epoch 0010 | Loss 0.6262 | Val F1(illicit) 0.522 | Thr 0.95\n",
      "[GAT   ] Epoch 0015 | Loss 0.5292 | Val F1(illicit) 0.462 | Thr 0.95\n",
      "[GAT   ] Epoch 0020 | Loss 0.4699 | Val F1(illicit) 0.622 | Thr 0.90\n",
      "[GAT   ] Epoch 0025 | Loss 0.4342 | Val F1(illicit) 0.654 | Thr 0.93\n",
      "[GAT   ] Epoch 0030 | Loss 0.4167 | Val F1(illicit) 0.615 | Thr 0.93\n",
      "[GAT   ] Epoch 0035 | Loss 0.4064 | Val F1(illicit) 0.657 | Thr 0.90\n",
      "[GAT   ] Epoch 0040 | Loss 0.3808 | Val F1(illicit) 0.689 | Thr 0.90\n",
      "[GAT   ] Epoch 0045 | Loss 0.3831 | Val F1(illicit) 0.703 | Thr 0.88\n",
      "[GAT   ] Epoch 0050 | Loss 0.3777 | Val F1(illicit) 0.694 | Thr 0.90\n",
      "[GAT   ] Epoch 0055 | Loss 0.3731 | Val F1(illicit) 0.685 | Thr 0.90\n",
      "[GAT   ] Epoch 0060 | Loss 0.3691 | Val F1(illicit) 0.701 | Thr 0.90\n",
      "[GAT   ] Epoch 0065 | Loss 0.3574 | Val F1(illicit) 0.707 | Thr 0.90\n",
      "[GAT   ] Epoch 0070 | Loss 0.3576 | Val F1(illicit) 0.710 | Thr 0.90\n",
      "[GAT   ] Epoch 0075 | Loss 0.3545 | Val F1(illicit) 0.710 | Thr 0.90\n",
      "[GAT   ] Epoch 0080 | Loss 0.3453 | Val F1(illicit) 0.711 | Thr 0.90\n",
      "[GAT   ] Epoch 0085 | Loss 0.3426 | Val F1(illicit) 0.704 | Thr 0.90\n",
      "[GAT   ] Epoch 0090 | Loss 0.3381 | Val F1(illicit) 0.706 | Thr 0.90\n",
      "[GAT   ] Epoch 0095 | Loss 0.3431 | Val F1(illicit) 0.711 | Thr 0.90\n",
      "[GAT   ] Epoch 0100 | Loss 0.3270 | Val F1(illicit) 0.699 | Thr 0.90\n",
      "[GAT   ] Epoch 0105 | Loss 0.3262 | Val F1(illicit) 0.716 | Thr 0.90\n",
      "[GAT   ] Epoch 0110 | Loss 0.3255 | Val F1(illicit) 0.734 | Thr 0.90\n",
      "[GAT   ] Epoch 0115 | Loss 0.3203 | Val F1(illicit) 0.724 | Thr 0.88\n",
      "[GAT   ] Epoch 0120 | Loss 0.3250 | Val F1(illicit) 0.727 | Thr 0.90\n",
      "[GAT   ] Epoch 0125 | Loss 0.3235 | Val F1(illicit) 0.739 | Thr 0.90\n",
      "[GAT   ] Epoch 0130 | Loss 0.3137 | Val F1(illicit) 0.736 | Thr 0.90\n",
      "[GAT   ] Epoch 0135 | Loss 0.3166 | Val F1(illicit) 0.743 | Thr 0.90\n",
      "[GAT   ] Epoch 0140 | Loss 0.3074 | Val F1(illicit) 0.735 | Thr 0.88\n",
      "[GAT   ] Epoch 0145 | Loss 0.3074 | Val F1(illicit) 0.745 | Thr 0.88\n",
      "[GAT   ] Epoch 0150 | Loss 0.3030 | Val F1(illicit) 0.751 | Thr 0.93\n",
      "[GAT   ] Epoch 0155 | Loss 0.3026 | Val F1(illicit) 0.751 | Thr 0.85\n",
      "[GAT   ] Epoch 0160 | Loss 0.3017 | Val F1(illicit) 0.749 | Thr 0.90\n",
      "[GAT   ] Epoch 0165 | Loss 0.2953 | Val F1(illicit) 0.760 | Thr 0.90\n",
      "[GAT   ] Epoch 0170 | Loss 0.2925 | Val F1(illicit) 0.772 | Thr 0.85\n",
      "[GAT   ] Epoch 0175 | Loss 0.2917 | Val F1(illicit) 0.776 | Thr 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAT   ] Epoch 0180 | Loss 0.2869 | Val F1(illicit) 0.768 | Thr 0.90\n",
      "[GAT   ] Epoch 0185 | Loss 0.2922 | Val F1(illicit) 0.771 | Thr 0.90\n",
      "[GAT   ] Epoch 0190 | Loss 0.2965 | Val F1(illicit) 0.791 | Thr 0.88\n",
      "[GAT   ] Epoch 0195 | Loss 0.2822 | Val F1(illicit) 0.782 | Thr 0.85\n",
      "[GAT   ] Epoch 0200 | Loss 0.2817 | Val F1(illicit) 0.794 | Thr 0.88\n",
      "[GAT   ] Epoch 0205 | Loss 0.2875 | Val F1(illicit) 0.799 | Thr 0.88\n",
      "[GAT   ] Epoch 0210 | Loss 0.2821 | Val F1(illicit) 0.789 | Thr 0.88\n",
      "[GAT   ] Epoch 0215 | Loss 0.2804 | Val F1(illicit) 0.792 | Thr 0.88\n",
      "[GAT   ] Epoch 0220 | Loss 0.2801 | Val F1(illicit) 0.780 | Thr 0.93\n",
      "[GAT   ] Epoch 0225 | Loss 0.2863 | Val F1(illicit) 0.785 | Thr 0.88\n",
      "[GAT   ] Epoch 0230 | Loss 0.2713 | Val F1(illicit) 0.799 | Thr 0.93\n",
      "[GAT   ] Epoch 0235 | Loss 0.2827 | Val F1(illicit) 0.797 | Thr 0.90\n",
      "[GAT   ] Epoch 0240 | Loss 0.2703 | Val F1(illicit) 0.803 | Thr 0.93\n",
      "[GAT   ] Epoch 0245 | Loss 0.2732 | Val F1(illicit) 0.794 | Thr 0.88\n",
      "[GAT   ] Epoch 0250 | Loss 0.2726 | Val F1(illicit) 0.802 | Thr 0.88\n",
      "[GAT   ] Epoch 0255 | Loss 0.2693 | Val F1(illicit) 0.810 | Thr 0.88\n",
      "[GAT   ] Epoch 0260 | Loss 0.2723 | Val F1(illicit) 0.821 | Thr 0.85\n",
      "[GAT   ] Epoch 0265 | Loss 0.2716 | Val F1(illicit) 0.824 | Thr 0.90\n",
      "[GAT   ] Epoch 0270 | Loss 0.2687 | Val F1(illicit) 0.819 | Thr 0.88\n",
      "[GAT   ] Epoch 0275 | Loss 0.2716 | Val F1(illicit) 0.821 | Thr 0.90\n",
      "[GAT   ] Epoch 0280 | Loss 0.2623 | Val F1(illicit) 0.824 | Thr 0.90\n",
      "[GAT   ] Epoch 0285 | Loss 0.2710 | Val F1(illicit) 0.822 | Thr 0.90\n",
      "[GAT   ] Epoch 0290 | Loss 0.2652 | Val F1(illicit) 0.823 | Thr 0.82\n",
      "[GAT   ] Epoch 0295 | Loss 0.2610 | Val F1(illicit) 0.818 | Thr 0.82\n",
      "[GAT   ] Epoch 0300 | Loss 0.2687 | Val F1(illicit) 0.823 | Thr 0.88\n",
      "[GAT   ] Epoch 0305 | Loss 0.2649 | Val F1(illicit) 0.831 | Thr 0.88\n",
      "[GAT   ] Epoch 0310 | Loss 0.2659 | Val F1(illicit) 0.824 | Thr 0.88\n",
      "[GAT   ] Epoch 0315 | Loss 0.2649 | Val F1(illicit) 0.839 | Thr 0.88\n",
      "[GAT   ] Epoch 0320 | Loss 0.2656 | Val F1(illicit) 0.829 | Thr 0.85\n",
      "[GAT   ] Epoch 0325 | Loss 0.2713 | Val F1(illicit) 0.832 | Thr 0.93\n",
      "[GAT   ] Epoch 0330 | Loss 0.2566 | Val F1(illicit) 0.823 | Thr 0.88\n",
      "[GAT   ] Epoch 0335 | Loss 0.2645 | Val F1(illicit) 0.829 | Thr 0.90\n",
      "[GAT   ] Epoch 0340 | Loss 0.2586 | Val F1(illicit) 0.823 | Thr 0.88\n",
      "[GAT   ] Epoch 0345 | Loss 0.2606 | Val F1(illicit) 0.833 | Thr 0.88\n",
      "[GAT   ] Epoch 0350 | Loss 0.2641 | Val F1(illicit) 0.826 | Thr 0.88\n",
      "[GAT   ] Epoch 0355 | Loss 0.2568 | Val F1(illicit) 0.841 | Thr 0.90\n",
      "[GAT   ] Epoch 0360 | Loss 0.2580 | Val F1(illicit) 0.838 | Thr 0.90\n",
      "[GAT   ] Epoch 0365 | Loss 0.2701 | Val F1(illicit) 0.824 | Thr 0.88\n",
      "[GAT   ] Epoch 0370 | Loss 0.2634 | Val F1(illicit) 0.835 | Thr 0.90\n",
      "[GAT   ] Epoch 0375 | Loss 0.2550 | Val F1(illicit) 0.842 | Thr 0.90\n",
      "[GAT   ] Epoch 0380 | Loss 0.2491 | Val F1(illicit) 0.833 | Thr 0.90\n",
      "[GAT   ] Epoch 0385 | Loss 0.2529 | Val F1(illicit) 0.830 | Thr 0.88\n",
      "[GAT   ] Epoch 0390 | Loss 0.2555 | Val F1(illicit) 0.841 | Thr 0.93\n",
      "[GAT   ] Epoch 0395 | Loss 0.2585 | Val F1(illicit) 0.846 | Thr 0.90\n",
      "[GAT   ] Epoch 0400 | Loss 0.2560 | Val F1(illicit) 0.846 | Thr 0.90\n",
      "[GAT   ] Epoch 0405 | Loss 0.2588 | Val F1(illicit) 0.848 | Thr 0.90\n",
      "[GAT   ] Epoch 0410 | Loss 0.2540 | Val F1(illicit) 0.852 | Thr 0.93\n",
      "[GAT   ] Epoch 0415 | Loss 0.2587 | Val F1(illicit) 0.831 | Thr 0.93\n",
      "[GAT   ] Epoch 0420 | Loss 0.2574 | Val F1(illicit) 0.850 | Thr 0.93\n",
      "[GAT   ] Epoch 0425 | Loss 0.2644 | Val F1(illicit) 0.848 | Thr 0.90\n",
      "[GAT   ] Epoch 0430 | Loss 0.2517 | Val F1(illicit) 0.845 | Thr 0.88\n",
      "[GAT   ] Epoch 0435 | Loss 0.2519 | Val F1(illicit) 0.837 | Thr 0.85\n",
      "[GAT   ] Epoch 0440 | Loss 0.2546 | Val F1(illicit) 0.847 | Thr 0.90\n",
      "[GAT   ] Epoch 0445 | Loss 0.2509 | Val F1(illicit) 0.835 | Thr 0.85\n",
      "[GAT   ] Epoch 0450 | Loss 0.2625 | Val F1(illicit) 0.841 | Thr 0.88\n",
      "[GAT   ] Epoch 0455 | Loss 0.2591 | Val F1(illicit) 0.854 | Thr 0.88\n",
      "[GAT   ] Epoch 0460 | Loss 0.2561 | Val F1(illicit) 0.860 | Thr 0.90\n",
      "[GAT   ] Epoch 0465 | Loss 0.2571 | Val F1(illicit) 0.852 | Thr 0.88\n",
      "[GAT   ] Epoch 0470 | Loss 0.2516 | Val F1(illicit) 0.851 | Thr 0.93\n",
      "[GAT   ] Epoch 0475 | Loss 0.2533 | Val F1(illicit) 0.845 | Thr 0.85\n",
      "[GAT   ] Epoch 0480 | Loss 0.2605 | Val F1(illicit) 0.838 | Thr 0.90\n",
      "[GAT   ] Epoch 0485 | Loss 0.2572 | Val F1(illicit) 0.858 | Thr 0.90\n",
      "[GAT   ] Epoch 0490 | Loss 0.2548 | Val F1(illicit) 0.845 | Thr 0.90\n",
      "[GAT   ] Epoch 0495 | Loss 0.2534 | Val F1(illicit) 0.851 | Thr 0.90\n",
      "[GAT   ] Epoch 0500 | Loss 0.2573 | Val F1(illicit) 0.857 | Thr 0.90\n",
      "[GAT   ] Epoch 0505 | Loss 0.2580 | Val F1(illicit) 0.857 | Thr 0.90\n",
      "[GAT   ] Epoch 0510 | Loss 0.2501 | Val F1(illicit) 0.844 | Thr 0.88\n",
      "Embeddings ready:\n",
      "GAT: (203769, 64)\n",
      "\n",
      "[GAT] TEST → F1(illicit): 0.534, Precision: 0.580, Recall: 0.495, Acc: 0.944, Micro-F1: 0.944\n",
      "TIP: Call gnn_train('arch_name') where arch_name ∈ [gcn, skip_gcn, gat, gatv2, sage, evolvegcn]\n",
      "\n",
      "=== Training GATV2 ===\n",
      "[GATV2 ] Epoch 0001 | Loss 1.7207 | Val F1(illicit) 0.357 | Thr 0.88\n",
      "[GATV2 ] Epoch 0005 | Loss 0.9359 | Val F1(illicit) 0.454 | Thr 0.95\n",
      "[GATV2 ] Epoch 0010 | Loss 0.6561 | Val F1(illicit) 0.521 | Thr 0.95\n",
      "[GATV2 ] Epoch 0015 | Loss 0.5371 | Val F1(illicit) 0.467 | Thr 0.95\n",
      "[GATV2 ] Epoch 0020 | Loss 0.4818 | Val F1(illicit) 0.563 | Thr 0.90\n",
      "[GATV2 ] Epoch 0025 | Loss 0.4557 | Val F1(illicit) 0.665 | Thr 0.90\n",
      "[GATV2 ] Epoch 0030 | Loss 0.4261 | Val F1(illicit) 0.684 | Thr 0.93\n",
      "[GATV2 ] Epoch 0035 | Loss 0.4108 | Val F1(illicit) 0.668 | Thr 0.88\n",
      "[GATV2 ] Epoch 0040 | Loss 0.3954 | Val F1(illicit) 0.671 | Thr 0.90\n",
      "[GATV2 ] Epoch 0045 | Loss 0.3762 | Val F1(illicit) 0.702 | Thr 0.90\n",
      "[GATV2 ] Epoch 0050 | Loss 0.3814 | Val F1(illicit) 0.722 | Thr 0.88\n",
      "[GATV2 ] Epoch 0055 | Loss 0.3664 | Val F1(illicit) 0.723 | Thr 0.90\n",
      "[GATV2 ] Epoch 0060 | Loss 0.3551 | Val F1(illicit) 0.717 | Thr 0.90\n",
      "[GATV2 ] Epoch 0065 | Loss 0.3528 | Val F1(illicit) 0.731 | Thr 0.90\n",
      "[GATV2 ] Epoch 0070 | Loss 0.3448 | Val F1(illicit) 0.747 | Thr 0.90\n",
      "[GATV2 ] Epoch 0075 | Loss 0.3378 | Val F1(illicit) 0.751 | Thr 0.90\n",
      "[GATV2 ] Epoch 0080 | Loss 0.3382 | Val F1(illicit) 0.750 | Thr 0.90\n",
      "[GATV2 ] Epoch 0085 | Loss 0.3251 | Val F1(illicit) 0.756 | Thr 0.90\n",
      "[GATV2 ] Epoch 0090 | Loss 0.3244 | Val F1(illicit) 0.756 | Thr 0.90\n",
      "[GATV2 ] Epoch 0095 | Loss 0.3260 | Val F1(illicit) 0.760 | Thr 0.90\n",
      "[GATV2 ] Epoch 0100 | Loss 0.3146 | Val F1(illicit) 0.760 | Thr 0.90\n",
      "[GATV2 ] Epoch 0105 | Loss 0.3146 | Val F1(illicit) 0.755 | Thr 0.90\n",
      "[GATV2 ] Epoch 0110 | Loss 0.3059 | Val F1(illicit) 0.759 | Thr 0.90\n",
      "[GATV2 ] Epoch 0115 | Loss 0.3009 | Val F1(illicit) 0.764 | Thr 0.93\n",
      "[GATV2 ] Epoch 0120 | Loss 0.3013 | Val F1(illicit) 0.768 | Thr 0.90\n",
      "[GATV2 ] Epoch 0125 | Loss 0.3044 | Val F1(illicit) 0.772 | Thr 0.90\n",
      "[GATV2 ] Epoch 0130 | Loss 0.2946 | Val F1(illicit) 0.774 | Thr 0.85\n",
      "[GATV2 ] Epoch 0135 | Loss 0.2933 | Val F1(illicit) 0.776 | Thr 0.90\n",
      "[GATV2 ] Epoch 0140 | Loss 0.2820 | Val F1(illicit) 0.777 | Thr 0.88\n",
      "[GATV2 ] Epoch 0145 | Loss 0.2886 | Val F1(illicit) 0.775 | Thr 0.88\n",
      "[GATV2 ] Epoch 0150 | Loss 0.2831 | Val F1(illicit) 0.782 | Thr 0.90\n",
      "[GATV2 ] Epoch 0155 | Loss 0.2812 | Val F1(illicit) 0.785 | Thr 0.90\n",
      "[GATV2 ] Epoch 0160 | Loss 0.2794 | Val F1(illicit) 0.781 | Thr 0.85\n",
      "[GATV2 ] Epoch 0165 | Loss 0.2806 | Val F1(illicit) 0.786 | Thr 0.90\n",
      "[GATV2 ] Epoch 0170 | Loss 0.2718 | Val F1(illicit) 0.787 | Thr 0.90\n",
      "[GATV2 ] Epoch 0175 | Loss 0.2741 | Val F1(illicit) 0.785 | Thr 0.90\n",
      "[GATV2 ] Epoch 0180 | Loss 0.2668 | Val F1(illicit) 0.788 | Thr 0.90\n",
      "[GATV2 ] Epoch 0185 | Loss 0.2735 | Val F1(illicit) 0.794 | Thr 0.90\n",
      "[GATV2 ] Epoch 0190 | Loss 0.2610 | Val F1(illicit) 0.788 | Thr 0.85\n",
      "[GATV2 ] Epoch 0195 | Loss 0.2635 | Val F1(illicit) 0.799 | Thr 0.88\n",
      "[GATV2 ] Epoch 0200 | Loss 0.2632 | Val F1(illicit) 0.786 | Thr 0.90\n",
      "[GATV2 ] Epoch 0205 | Loss 0.2616 | Val F1(illicit) 0.806 | Thr 0.88\n",
      "[GATV2 ] Epoch 0210 | Loss 0.2625 | Val F1(illicit) 0.810 | Thr 0.93\n",
      "[GATV2 ] Epoch 0215 | Loss 0.2610 | Val F1(illicit) 0.794 | Thr 0.90\n",
      "[GATV2 ] Epoch 0220 | Loss 0.2533 | Val F1(illicit) 0.816 | Thr 0.88\n",
      "[GATV2 ] Epoch 0225 | Loss 0.2590 | Val F1(illicit) 0.807 | Thr 0.93\n",
      "[GATV2 ] Epoch 0230 | Loss 0.2519 | Val F1(illicit) 0.798 | Thr 0.88\n",
      "[GATV2 ] Epoch 0235 | Loss 0.2555 | Val F1(illicit) 0.796 | Thr 0.90\n",
      "[GATV2 ] Epoch 0240 | Loss 0.2523 | Val F1(illicit) 0.796 | Thr 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GATV2 ] Epoch 0245 | Loss 0.2541 | Val F1(illicit) 0.808 | Thr 0.90\n",
      "[GATV2 ] Epoch 0250 | Loss 0.2464 | Val F1(illicit) 0.794 | Thr 0.85\n",
      "[GATV2 ] Epoch 0255 | Loss 0.2444 | Val F1(illicit) 0.791 | Thr 0.90\n",
      "[GATV2 ] Epoch 0260 | Loss 0.2466 | Val F1(illicit) 0.802 | Thr 0.88\n",
      "[GATV2 ] Epoch 0265 | Loss 0.2478 | Val F1(illicit) 0.790 | Thr 0.90\n",
      "[GATV2 ] Epoch 0270 | Loss 0.2506 | Val F1(illicit) 0.798 | Thr 0.90\n",
      "Embeddings ready:\n",
      "GATv2: (203769, 64)\n",
      "\n",
      "[GATV2] TEST → F1(illicit): 0.537, Precision: 0.494, Recall: 0.588, Acc: 0.934, Micro-F1: 0.934\n",
      "TIP: Call gnn_train('arch_name') where arch_name ∈ [gcn, skip_gcn, gat, gatv2, sage, evolvegcn]\n",
      "\n",
      "=== Training SAGE ===\n",
      "[SAGE  ] Epoch 0001 | Loss 0.7016 | Val F1(illicit) 0.420 | Thr 0.20\n",
      "[SAGE  ] Epoch 0005 | Loss 0.3386 | Val F1(illicit) 0.554 | Thr 0.70\n",
      "[SAGE  ] Epoch 0010 | Loss 0.2570 | Val F1(illicit) 0.593 | Thr 0.90\n",
      "[SAGE  ] Epoch 0015 | Loss 0.2057 | Val F1(illicit) 0.754 | Thr 0.95\n",
      "[SAGE  ] Epoch 0020 | Loss 0.1713 | Val F1(illicit) 0.751 | Thr 0.95\n",
      "[SAGE  ] Epoch 0025 | Loss 0.1542 | Val F1(illicit) 0.737 | Thr 0.90\n",
      "[SAGE  ] Epoch 0030 | Loss 0.1315 | Val F1(illicit) 0.768 | Thr 0.75\n",
      "[SAGE  ] Epoch 0035 | Loss 0.1337 | Val F1(illicit) 0.812 | Thr 0.68\n",
      "[SAGE  ] Epoch 0040 | Loss 0.1177 | Val F1(illicit) 0.791 | Thr 0.53\n",
      "[SAGE  ] Epoch 0045 | Loss 0.1034 | Val F1(illicit) 0.828 | Thr 0.65\n",
      "[SAGE  ] Epoch 0050 | Loss 0.0893 | Val F1(illicit) 0.832 | Thr 0.62\n",
      "[SAGE  ] Epoch 0055 | Loss 0.0777 | Val F1(illicit) 0.860 | Thr 0.65\n",
      "[SAGE  ] Epoch 0060 | Loss 0.0716 | Val F1(illicit) 0.854 | Thr 0.70\n",
      "[SAGE  ] Epoch 0065 | Loss 0.0677 | Val F1(illicit) 0.864 | Thr 0.62\n",
      "[SAGE  ] Epoch 0070 | Loss 0.0890 | Val F1(illicit) 0.847 | Thr 0.62\n",
      "[SAGE  ] Epoch 0075 | Loss 0.0751 | Val F1(illicit) 0.880 | Thr 0.88\n",
      "[SAGE  ] Epoch 0080 | Loss 0.0721 | Val F1(illicit) 0.885 | Thr 0.85\n",
      "[SAGE  ] Epoch 0085 | Loss 0.0796 | Val F1(illicit) 0.881 | Thr 0.70\n",
      "[SAGE  ] Epoch 0090 | Loss 0.0672 | Val F1(illicit) 0.898 | Thr 0.82\n",
      "[SAGE  ] Epoch 0095 | Loss 0.0651 | Val F1(illicit) 0.892 | Thr 0.85\n",
      "[SAGE  ] Epoch 0100 | Loss 0.0561 | Val F1(illicit) 0.891 | Thr 0.93\n",
      "[SAGE  ] Epoch 0105 | Loss 0.0486 | Val F1(illicit) 0.881 | Thr 0.85\n",
      "[SAGE  ] Epoch 0110 | Loss 0.0684 | Val F1(illicit) 0.888 | Thr 0.95\n",
      "[SAGE  ] Epoch 0115 | Loss 0.0572 | Val F1(illicit) 0.898 | Thr 0.95\n",
      "[SAGE  ] Epoch 0120 | Loss 0.0503 | Val F1(illicit) 0.904 | Thr 0.93\n",
      "[SAGE  ] Epoch 0125 | Loss 0.0483 | Val F1(illicit) 0.892 | Thr 0.90\n",
      "[SAGE  ] Epoch 0130 | Loss 0.0495 | Val F1(illicit) 0.902 | Thr 0.82\n",
      "[SAGE  ] Epoch 0135 | Loss 0.0519 | Val F1(illicit) 0.886 | Thr 0.95\n",
      "[SAGE  ] Epoch 0140 | Loss 0.0489 | Val F1(illicit) 0.884 | Thr 0.90\n",
      "[SAGE  ] Epoch 0145 | Loss 0.0551 | Val F1(illicit) 0.857 | Thr 0.93\n",
      "[SAGE  ] Epoch 0150 | Loss 0.0507 | Val F1(illicit) 0.832 | Thr 0.82\n",
      "[SAGE  ] Epoch 0155 | Loss 0.0454 | Val F1(illicit) 0.858 | Thr 0.90\n",
      "[SAGE  ] Epoch 0160 | Loss 0.0462 | Val F1(illicit) 0.884 | Thr 0.90\n",
      "[SAGE  ] Epoch 0165 | Loss 0.0463 | Val F1(illicit) 0.890 | Thr 0.95\n",
      "[SAGE  ] Epoch 0170 | Loss 0.0461 | Val F1(illicit) 0.873 | Thr 0.77\n",
      "Embeddings ready:\n",
      "Sage: (203769, 64)\n",
      "\n",
      "[SAGE] TEST → F1(illicit): 0.446, Precision: 0.323, Recall: 0.718, Acc: 0.884, Micro-F1: 0.884\n",
      "TIP: Call gnn_train('arch_name') where arch_name ∈ [gcn, skip_gcn, gat, gatv2, sage, evolvegcn]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Micro-F1</th>\n",
       "      <th>Val best F1</th>\n",
       "      <th>Best thr</th>\n",
       "      <th>Embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SKIP_GCN</td>\n",
       "      <td>0.606314</td>\n",
       "      <td>0.609711</td>\n",
       "      <td>0.602955</td>\n",
       "      <td>0.949130</td>\n",
       "      <td>0.949130</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.450</td>\n",
       "      <td>(203769, 64)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GATV2</td>\n",
       "      <td>0.537099</td>\n",
       "      <td>0.494182</td>\n",
       "      <td>0.588181</td>\n",
       "      <td>0.934133</td>\n",
       "      <td>0.934133</td>\n",
       "      <td>0.816169</td>\n",
       "      <td>0.875</td>\n",
       "      <td>(203769, 64)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GCN</td>\n",
       "      <td>0.535433</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.596491</td>\n",
       "      <td>0.932753</td>\n",
       "      <td>0.932753</td>\n",
       "      <td>0.826291</td>\n",
       "      <td>0.725</td>\n",
       "      <td>(203769, 64)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GAT</td>\n",
       "      <td>0.534131</td>\n",
       "      <td>0.580087</td>\n",
       "      <td>0.494922</td>\n",
       "      <td>0.943911</td>\n",
       "      <td>0.943911</td>\n",
       "      <td>0.860258</td>\n",
       "      <td>0.900</td>\n",
       "      <td>(203769, 64)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SAGE</td>\n",
       "      <td>0.446101</td>\n",
       "      <td>0.323493</td>\n",
       "      <td>0.718375</td>\n",
       "      <td>0.884103</td>\n",
       "      <td>0.884103</td>\n",
       "      <td>0.904110</td>\n",
       "      <td>0.925</td>\n",
       "      <td>(203769, 64)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model        F1  Precision    Recall  Accuracy  Micro-F1  Val best F1  \\\n",
       "0  SKIP_GCN  0.606314   0.609711  0.602955  0.949130  0.949130     0.867470   \n",
       "1     GATV2  0.537099   0.494182  0.588181  0.934133  0.934133     0.816169   \n",
       "2       GCN  0.535433   0.485714  0.596491  0.932753  0.932753     0.826291   \n",
       "3       GAT  0.534131   0.580087  0.494922  0.943911  0.943911     0.860258   \n",
       "4      SAGE  0.446101   0.323493  0.718375  0.884103  0.884103     0.904110   \n",
       "\n",
       "   Best thr    Embeddings  \n",
       "0     0.450  (203769, 64)  \n",
       "1     0.875  (203769, 64)  \n",
       "2     0.725  (203769, 64)  \n",
       "3     0.900  (203769, 64)  \n",
       "4     0.925  (203769, 64)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Train all (except EvolveGCN)\n",
    "arch_list = [\"gcn\", \"skip_gcn\", \"gat\", \"gatv2\", \"sage\"]\n",
    "all_results = {}\n",
    "for arch in arch_list:\n",
    "    print(f\"\\n=== Training {arch.upper()} ===\")\n",
    "    all_results[arch] = gnn_train(arch)\n",
    "\n",
    "# 2) Build a summary table\n",
    "rows = []\n",
    "for arch, r in all_results.items():\n",
    "    m = r[\"test_metrics\"]\n",
    "    rows.append({\n",
    "        \"Model\": arch.upper(),\n",
    "        \"F1\": m[\"F1\"],\n",
    "        \"Precision\": m[\"P\"],\n",
    "        \"Recall\": m[\"R\"],\n",
    "        \"Accuracy\": m[\"Acc\"],\n",
    "        \"Micro-F1\": m[\"microF1\"],\n",
    "        \"Val best F1\": r[\"val_best_F1\"],\n",
    "        \"Best thr\": r[\"best_threshold\"],\n",
    "        \"Embeddings\": r[\"embeddings_shape\"],\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(\"F1\", ascending=False).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa2d90d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
