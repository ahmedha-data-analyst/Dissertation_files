{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d55ddfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Elliptic dataset...\n",
      "Loaded 203,769 transactions and 234,355 edges\n",
      "\n",
      "K-Fold Configuration (Fold 1/5):\n",
      "  Train: 33,525 | Val: 3,726 | Test: 9,313\n",
      "  Train class balance: 3,272 illicit / 33,525 total (9.8%)\n",
      "  Graph: 203,769 nodes, 672,479 edges\n",
      "\n",
      "============================================================\n",
      "Training GNN Models with K-Fold Split\n",
      "============================================================\n",
      "\n",
      "Training GCN...\n",
      "----------------------------------------\n",
      "[GCN] Epoch    0 | Loss: 0.7259 | Val F1: 0.3827 | Thr: 0.47\n",
      "[GCN] Epoch   20 | Loss: 0.2721 | Val F1: 0.7411 | Thr: 0.57\n",
      "[GCN] Epoch   40 | Loss: 0.1940 | Val F1: 0.7791 | Thr: 0.70\n",
      "[GCN] Epoch   60 | Loss: 0.1623 | Val F1: 0.8118 | Thr: 0.82\n",
      "[GCN] Epoch  100 | Loss: 0.1227 | Val F1: 0.8493 | Thr: 0.82\n",
      "[GCN] Epoch  120 | Loss: 0.1205 | Val F1: 0.8579 | Thr: 0.90\n",
      "[GCN] Early stopping at epoch 200\n",
      "✓ Saved embeddings to global variable: gcn_embeddings (203769, 64)\n",
      "\n",
      "Training SKIP_GCN...\n",
      "----------------------------------------\n",
      "[SKIP_GCN] Epoch    0 | Loss: 0.7069 | Val F1: 0.5528 | Thr: 0.62\n",
      "[SKIP_GCN] Epoch   20 | Loss: 0.2017 | Val F1: 0.8237 | Thr: 0.77\n",
      "[SKIP_GCN] Epoch   40 | Loss: 0.1358 | Val F1: 0.8710 | Thr: 0.68\n",
      "[SKIP_GCN] Epoch   60 | Loss: 0.1117 | Val F1: 0.8920 | Thr: 0.80\n",
      "[SKIP_GCN] Epoch   80 | Loss: 0.0958 | Val F1: 0.9070 | Thr: 0.88\n",
      "[SKIP_GCN] Epoch  140 | Loss: 0.0752 | Val F1: 0.9220 | Thr: 0.90\n",
      "[SKIP_GCN] Early stopping at epoch 225\n",
      "✓ Saved embeddings to global variable: skip_gcn_embeddings (203769, 64)\n",
      "\n",
      "Training GAT...\n",
      "----------------------------------------\n",
      "[GAT] Epoch    0 | Loss: 1.1901 | Val F1: 0.4587 | Thr: 0.95\n",
      "[GAT] Epoch   40 | Loss: 0.3927 | Val F1: 0.7589 | Thr: 0.82\n",
      "[GAT] Epoch   60 | Loss: 0.3777 | Val F1: 0.7801 | Thr: 0.88\n",
      "[GAT] Epoch  140 | Loss: 0.3334 | Val F1: 0.8098 | Thr: 0.88\n",
      "[GAT] Epoch  200 | Loss: 0.3099 | Val F1: 0.8354 | Thr: 0.88\n",
      "[GAT] Epoch  220 | Loss: 0.3052 | Val F1: 0.8428 | Thr: 0.85\n",
      "[GAT] Epoch  280 | Loss: 0.3011 | Val F1: 0.8459 | Thr: 0.90\n",
      "[GAT] Epoch  360 | Loss: 0.2969 | Val F1: 0.8559 | Thr: 0.88\n",
      "[GAT] Epoch  380 | Loss: 0.2852 | Val F1: 0.8625 | Thr: 0.93\n",
      "[GAT] Early stopping at epoch 430\n",
      "✓ Saved embeddings to global variable: gat_embeddings (203769, 64)\n",
      "\n",
      "Training GATV2...\n",
      "----------------------------------------\n",
      "[GATV2] Epoch    0 | Loss: 1.0453 | Val F1: 0.3099 | Thr: 0.95\n",
      "[GATV2] Epoch   20 | Loss: 0.4349 | Val F1: 0.7151 | Thr: 0.88\n",
      "[GATV2] Epoch   40 | Loss: 0.3799 | Val F1: 0.7721 | Thr: 0.85\n",
      "[GATV2] Epoch   60 | Loss: 0.3535 | Val F1: 0.7834 | Thr: 0.77\n",
      "[GATV2] Epoch   80 | Loss: 0.3383 | Val F1: 0.7983 | Thr: 0.82\n",
      "[GATV2] Epoch  120 | Loss: 0.3104 | Val F1: 0.8154 | Thr: 0.85\n",
      "[GATV2] Epoch  140 | Loss: 0.3083 | Val F1: 0.8239 | Thr: 0.90\n",
      "[GATV2] Epoch  160 | Loss: 0.2951 | Val F1: 0.8336 | Thr: 0.88\n",
      "[GATV2] Epoch  180 | Loss: 0.2814 | Val F1: 0.8415 | Thr: 0.88\n",
      "[GATV2] Epoch  220 | Loss: 0.2808 | Val F1: 0.8469 | Thr: 0.88\n",
      "[GATV2] Early stopping at epoch 285\n",
      "✓ Saved embeddings to global variable: gatv2_embeddings (203769, 64)\n",
      "\n",
      "Training SAGE...\n",
      "----------------------------------------\n",
      "[SAGE] Epoch    0 | Loss: 0.7155 | Val F1: 0.4331 | Thr: 0.35\n",
      "[SAGE] Epoch   20 | Loss: 0.2179 | Val F1: 0.8045 | Thr: 0.77\n",
      "[SAGE] Epoch   60 | Loss: 0.1097 | Val F1: 0.8910 | Thr: 0.62\n",
      "[SAGE] Epoch   80 | Loss: 0.0845 | Val F1: 0.9016 | Thr: 0.62\n",
      "[SAGE] Early stopping at epoch 165\n",
      "✓ Saved embeddings to global variable: sage_embeddings (203769, 64)\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS - K-Fold GNN Pipeline with Embeddings\n",
      "================================================================================\n",
      "      Model      F1  Precision  Recall  Accuracy  Micro-F1  Val best F1  Best thr    Embeddings\n",
      "0  SKIP_GCN  0.9162     0.9440  0.8900    0.9841    0.9841       0.9246      0.88  (203769, 64)\n",
      "1      SAGE  0.9152     0.9481  0.8845    0.9840    0.9840       0.9191      0.90  (203769, 64)\n",
      "2     GATV2  0.8670     0.8808  0.8537    0.9744    0.9744       0.8543      0.90  (203769, 64)\n",
      "3       GCN  0.8657     0.8888  0.8438    0.9744    0.9744       0.8693      0.88  (203769, 64)\n",
      "4       GAT  0.8612     0.8955  0.8295    0.9739    0.9739       0.8625      0.93  (203769, 64)\n",
      "\n",
      "================================================================================\n",
      "Embeddings Saved (Accessible as Global Variables)\n",
      "================================================================================\n",
      "  gcn_embeddings: shape (203769, 64)\n",
      "  skip_gcn_embeddings: shape (203769, 64)\n",
      "  gat_embeddings: shape (203769, 64)\n",
      "  gatv2_embeddings: shape (203769, 64)\n",
      "  sage_embeddings: shape (203769, 64)\n",
      "\n",
      "================================================================================\n",
      "Configuration Summary\n",
      "================================================================================\n",
      "  K-Fold splits: 5\n",
      "  Current fold: 1/5\n",
      "  Validation ratio: 0.1\n",
      "  Train-only z-score normalization: ✓\n",
      "  Class-weighted loss: ✓\n",
      "  Architecture-specific hyperparameters: ✓\n",
      "  Graph preprocessing (undirected + self-loops): ✓\n",
      "\n",
      "Usage Example:\n",
      "  # Access embeddings for baseline models\n",
      "  X_enhanced = np.concatenate([X_original, sage_embeddings], axis=1)\n",
      "  # Train baseline with enhanced features\n",
      "  rf_enhanced = RandomForest().fit(X_enhanced[train], y[train])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete K-Fold GNN Pipeline for Elliptic Dataset with Embedding Extraction\n",
    "===========================================================================\n",
    "This function trains all GNN models using K-fold cross-validation and saves\n",
    "node embeddings for use with baseline ML models.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv, GATv2Conv, SAGEConv\n",
    "from torch_geometric.utils import to_undirected, add_self_loops\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def run_elliptic_kfold_with_embeddings(\n",
    "    n_splits=5,\n",
    "    fold_id=1,          # 1..n_splits\n",
    "    val_ratio=0.10,     # stratified holdout from train\n",
    "    epochs=1000,\n",
    "    patience=50,\n",
    "    random_state=42,\n",
    "    device=None,\n",
    "    save_embeddings=True  # Whether to save embeddings globally\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete K-fold GNN pipeline with embedding extraction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_splits : int\n",
    "        Number of K-fold splits\n",
    "    fold_id : int\n",
    "        Which fold to use as test (1 to n_splits)\n",
    "    val_ratio : float\n",
    "        Proportion of training data to use for validation\n",
    "    epochs : int\n",
    "        Maximum training epochs\n",
    "    patience : int\n",
    "        Early stopping patience\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    device : torch.device\n",
    "        Device for computation\n",
    "    save_embeddings : bool\n",
    "        Whether to save node embeddings as global variables\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df_results : DataFrame\n",
    "        Results for all models\n",
    "    embeddings_dict : dict\n",
    "        Dictionary of embeddings {model_name: embeddings_array}\n",
    "    \n",
    "    Global Variables Created:\n",
    "    ------------------------\n",
    "    gcn_embeddings, skip_gcn_embeddings, gat_embeddings, \n",
    "    gatv2_embeddings, sage_embeddings : numpy arrays of shape (N, 64)\n",
    "    \"\"\"\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # ============================================\n",
    "    # STEP 1: Load Data (Custom Prep, No Normalization Yet)\n",
    "    # ============================================\n",
    "    \n",
    "    def load_elliptic_custom():\n",
    "        \"\"\"Load Elliptic data without normalization.\"\"\"\n",
    "        # Load raw files\n",
    "        features_path = 'elliptic_txs_features.csv'\n",
    "        classes_path = 'elliptic_txs_classes.csv'\n",
    "        edges_path = 'elliptic_txs_edgelist.csv'\n",
    "        \n",
    "        # Load features\n",
    "        df_features = pd.read_csv(features_path, header=None)\n",
    "        \n",
    "        # Define column names\n",
    "        col_names = ['tx_id', 'time_step']\n",
    "        col_names += [f'local_{i}' for i in range(1, 94)]\n",
    "        col_names += [f'aggregated_{i}' for i in range(1, 73)]\n",
    "        df_features.columns = col_names\n",
    "        \n",
    "        # Fix types\n",
    "        df_features['tx_id'] = df_features['tx_id'].astype(int)\n",
    "        df_features['time_step'] = df_features['time_step'].astype(int)\n",
    "        \n",
    "        # Load classes and merge\n",
    "        df_classes = pd.read_csv(classes_path)\n",
    "        df_features = df_features.merge(\n",
    "            df_classes.rename(columns={'txId': 'tx_id'}),\n",
    "            on='tx_id',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Load edges\n",
    "        df_edges = pd.read_csv(edges_path)\n",
    "        df_edges['txId1'] = df_edges['txId1'].astype(int)\n",
    "        df_edges['txId2'] = df_edges['txId2'].astype(int)\n",
    "        \n",
    "        return df_features, df_edges\n",
    "    \n",
    "    print(\"Loading Elliptic dataset...\")\n",
    "    df_features, df_edges = load_elliptic_custom()\n",
    "    print(f\"Loaded {len(df_features):,} transactions and {len(df_edges):,} edges\")\n",
    "    \n",
    "    # ============================================\n",
    "    # STEP 2: Create PyG Data with K-Fold Splits\n",
    "    # ============================================\n",
    "    \n",
    "    def create_pyg_data_kfold(df_features, df_edges, n_splits, fold_id, val_ratio, random_state):\n",
    "        \"\"\"Convert to PyG format with K-fold splits.\"\"\"\n",
    "        \n",
    "        # Create node index mapping\n",
    "        tx_id_to_idx = {tx_id: idx for idx, tx_id in enumerate(df_features['tx_id'].values)}\n",
    "        \n",
    "        # Extract feature columns\n",
    "        feature_cols = [col for col in df_features.columns \n",
    "                       if col.startswith(('local_', 'aggregated_'))]\n",
    "        x = torch.tensor(df_features[feature_cols].values, dtype=torch.float)\n",
    "        \n",
    "        # Create labels: 0=licit, 1=illicit, 2=unknown\n",
    "        y = torch.full((len(df_features),), 2, dtype=torch.long)\n",
    "        \n",
    "        # Map string class to integers\n",
    "        class_values = df_features['class'].astype(str).values\n",
    "        y[class_values == '2'] = 0  # licit\n",
    "        y[class_values == '1'] = 1  # illicit\n",
    "        \n",
    "        # Create edge index (undirected + self-loops)\n",
    "        edge_list = []\n",
    "        for _, row in df_edges.iterrows():\n",
    "            if row['txId1'] in tx_id_to_idx and row['txId2'] in tx_id_to_idx:\n",
    "                idx1 = tx_id_to_idx[row['txId1']]\n",
    "                idx2 = tx_id_to_idx[row['txId2']]\n",
    "                edge_list.append([idx1, idx2])\n",
    "        \n",
    "        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "        edge_index = to_undirected(edge_index, num_nodes=len(df_features))\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=len(df_features))\n",
    "        \n",
    "        # Time steps\n",
    "        time_steps = torch.tensor(df_features['time_step'].values, dtype=torch.long)\n",
    "        \n",
    "        # ============ K-FOLD SPLITTING ============\n",
    "        # Get labeled indices only\n",
    "        labeled_indices = torch.where(y != 2)[0].numpy()\n",
    "        y_labeled = y[labeled_indices].numpy()\n",
    "        \n",
    "        # Create stratified K-fold splits\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        splits = list(skf.split(np.zeros_like(y_labeled), y_labeled))\n",
    "        \n",
    "        # Get train and test indices for the specified fold\n",
    "        train_fold_idx, test_fold_idx = splits[fold_id - 1]\n",
    "        train_labeled = labeled_indices[train_fold_idx]\n",
    "        test_labeled = labeled_indices[test_fold_idx]\n",
    "        \n",
    "        # Create validation split from training data\n",
    "        if val_ratio > 0:\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=random_state)\n",
    "            y_train_labeled = y[train_labeled].numpy()\n",
    "            train_keep_idx, val_idx = next(sss.split(np.zeros_like(y_train_labeled), y_train_labeled))\n",
    "            \n",
    "            final_train = train_labeled[train_keep_idx]\n",
    "            val_indices = train_labeled[val_idx]\n",
    "        else:\n",
    "            final_train = train_labeled\n",
    "            val_indices = np.array([], dtype=int)\n",
    "        \n",
    "        # Create boolean masks for all nodes\n",
    "        N = len(df_features)\n",
    "        train_mask = torch.zeros(N, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(N, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(N, dtype=torch.bool)\n",
    "        \n",
    "        train_mask[torch.tensor(final_train)] = True\n",
    "        if val_indices.size > 0:\n",
    "            val_mask[torch.tensor(val_indices)] = True\n",
    "        test_mask[torch.tensor(test_labeled)] = True\n",
    "        \n",
    "        # Verify masks are disjoint\n",
    "        assert (train_mask & val_mask).sum() == 0, \"Train and val masks overlap!\"\n",
    "        assert (train_mask & test_mask).sum() == 0, \"Train and test masks overlap!\"\n",
    "        assert (val_mask & test_mask).sum() == 0, \"Val and test masks overlap!\"\n",
    "        \n",
    "        # ============ Z-SCORE NORMALIZATION (TRAIN ONLY) ============\n",
    "        with torch.no_grad():\n",
    "            train_features = x[train_mask]\n",
    "            mu = train_features.mean(0, keepdim=True)\n",
    "            std = train_features.std(0, keepdim=True).clamp_min(1e-6)\n",
    "            x = (x - mu) / std\n",
    "        \n",
    "        # Create PyG Data object\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            y=y,\n",
    "            train_mask=train_mask,\n",
    "            val_mask=val_mask,\n",
    "            test_mask=test_mask,\n",
    "            time_steps=time_steps,\n",
    "            num_nodes=N\n",
    "        )\n",
    "        \n",
    "        # Print statistics\n",
    "        n_train = int(train_mask.sum())\n",
    "        n_val = int(val_mask.sum())\n",
    "        n_test = int(test_mask.sum())\n",
    "        \n",
    "        train_illicit = int((y[train_mask] == 1).sum())\n",
    "        train_licit = int((y[train_mask] == 0).sum())\n",
    "        illicit_pct = (train_illicit / (train_illicit + train_licit)) * 100\n",
    "        \n",
    "        print(f\"\\nK-Fold Configuration (Fold {fold_id}/{n_splits}):\")\n",
    "        print(f\"  Train: {n_train:,} | Val: {n_val:,} | Test: {n_test:,}\")\n",
    "        print(f\"  Train class balance: {train_illicit:,} illicit / {train_illicit + train_licit:,} total ({illicit_pct:.1f}%)\")\n",
    "        print(f\"  Graph: {N:,} nodes, {edge_index.size(1):,} edges\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    # Create data with K-fold splits\n",
    "    data = create_pyg_data_kfold(df_features, df_edges, n_splits, fold_id, val_ratio, random_state)\n",
    "    data = data.to(device)\n",
    "    \n",
    "    # ============================================\n",
    "    # STEP 3: Model Definitions (PyG-Matched)\n",
    "    # ============================================\n",
    "    \n",
    "    class GCNNet_Matched(nn.Module):\n",
    "        def __init__(self, in_dim, hidden=128, dropout=0.5):\n",
    "            super().__init__()\n",
    "            self.conv1 = GCNConv(in_dim, hidden, cached=True, normalize=True)\n",
    "            self.bn1 = nn.BatchNorm1d(hidden)\n",
    "            self.conv2 = GCNConv(hidden, 64, cached=True, normalize=True)\n",
    "            self.bn2 = nn.BatchNorm1d(64)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.act = nn.ReLU()\n",
    "            self.head = nn.Linear(64, 2)\n",
    "            \n",
    "        def forward(self, x, edge_index):\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = self.act(self.bn1(h))\n",
    "            h = self.dropout(h)\n",
    "            h = self.conv2(h, edge_index)\n",
    "            h = self.act(self.bn2(h))\n",
    "            h = self.dropout(h)\n",
    "            self.embeddings = h\n",
    "            return self.head(h)\n",
    "    \n",
    "    class SkipGCNNet_Matched(nn.Module):\n",
    "        def __init__(self, in_dim, hidden=128, dropout=0.5):\n",
    "            super().__init__()\n",
    "            self.in_proj = nn.Linear(in_dim, 64, bias=False)\n",
    "            self.conv1 = GCNConv(in_dim, hidden)\n",
    "            self.bn1 = nn.BatchNorm1d(hidden)\n",
    "            self.conv2 = GCNConv(hidden, 64)\n",
    "            self.bn2 = nn.BatchNorm1d(64)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.act = nn.ReLU()\n",
    "            self.head = nn.Linear(64, 2)\n",
    "            \n",
    "        def forward(self, x, edge_index):\n",
    "            skip = self.in_proj(x)\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = self.act(self.bn1(h))\n",
    "            h = self.dropout(h)\n",
    "            h = self.conv2(h, edge_index)\n",
    "            h = self.bn2(h)\n",
    "            h = self.act(h + skip)\n",
    "            h = self.dropout(h)\n",
    "            self.embeddings = h\n",
    "            return self.head(h)\n",
    "    \n",
    "    class GATNet_Matched(nn.Module):\n",
    "        def __init__(self, in_dim, hidden=128, heads=4, dropout=0.5):\n",
    "            super().__init__()\n",
    "            self.conv1 = GATConv(in_dim, hidden, heads=heads, concat=True,\n",
    "                                dropout=dropout, add_self_loops=False)\n",
    "            self.ln1 = nn.LayerNorm(hidden * heads)\n",
    "            self.act = nn.ELU()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.conv2 = GATConv(hidden * heads, 2, heads=1, concat=False,\n",
    "                                dropout=dropout, add_self_loops=False)\n",
    "            self.emb_proj = nn.Linear(hidden * heads, 64, bias=False)\n",
    "            self.emb_ln = nn.LayerNorm(64)\n",
    "            \n",
    "        def forward(self, x, edge_index):\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = self.act(self.ln1(h))\n",
    "            h = self.dropout(h)\n",
    "            logits = self.conv2(h, edge_index)\n",
    "            self.embeddings = self.emb_ln(self.emb_proj(h))\n",
    "            return logits\n",
    "    \n",
    "    class GATv2Net_Matched(nn.Module):\n",
    "        def __init__(self, in_dim, hidden=128, heads=4, dropout=0.5):\n",
    "            super().__init__()\n",
    "            self.conv1 = GATv2Conv(in_dim, hidden, heads=heads, concat=True,\n",
    "                                  dropout=dropout, add_self_loops=False)\n",
    "            self.ln1 = nn.LayerNorm(hidden * heads)\n",
    "            self.act = nn.ELU()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.conv2 = GATv2Conv(hidden * heads, 2, heads=1, concat=False,\n",
    "                                  dropout=dropout, add_self_loops=False)\n",
    "            self.emb_proj = nn.Linear(hidden * heads, 64, bias=False)\n",
    "            self.emb_ln = nn.LayerNorm(64)\n",
    "            \n",
    "        def forward(self, x, edge_index):\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = self.act(self.ln1(h))\n",
    "            h = self.dropout(h)\n",
    "            logits = self.conv2(h, edge_index)\n",
    "            self.embeddings = self.emb_ln(self.emb_proj(h))\n",
    "            return logits\n",
    "    \n",
    "    class SAGENet_Matched(nn.Module):\n",
    "        def __init__(self, in_dim, hidden=256, dropout=0.5):\n",
    "            super().__init__()\n",
    "            self.conv1 = SAGEConv(in_dim, hidden)\n",
    "            self.bn1 = nn.BatchNorm1d(hidden)\n",
    "            self.conv2 = SAGEConv(hidden, 64)\n",
    "            self.bn2 = nn.BatchNorm1d(64)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.act = nn.ReLU()\n",
    "            self.head = nn.Linear(64, 2)\n",
    "            \n",
    "        def forward(self, x, edge_index):\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = self.act(self.bn1(h))\n",
    "            h = self.dropout(h)\n",
    "            h = self.conv2(h, edge_index)\n",
    "            h = self.act(self.bn2(h))\n",
    "            h = self.dropout(h)\n",
    "            self.embeddings = h\n",
    "            return self.head(h)\n",
    "    \n",
    "    # ============================================\n",
    "    # STEP 4: Training Function with Embedding Extraction\n",
    "    # ============================================\n",
    "    \n",
    "    def train_gnn_with_embeddings(data, arch='gcn', epochs=1000, patience=50, \n",
    "                                  lr=None, weight_decay=5e-4, grad_clip=None):\n",
    "        \"\"\"Train GNN and extract embeddings.\"\"\"\n",
    "        \n",
    "        # Architecture configurations\n",
    "        arch_configs = {\n",
    "            'gcn': {\n",
    "                'model': GCNNet_Matched(data.x.size(1)),\n",
    "                'lr': 0.01,\n",
    "                'grad_clip': 2.0\n",
    "            },\n",
    "            'skip_gcn': {\n",
    "                'model': SkipGCNNet_Matched(data.x.size(1)),\n",
    "                'lr': 0.01,\n",
    "                'grad_clip': 2.0\n",
    "            },\n",
    "            'gat': {\n",
    "                'model': GATNet_Matched(data.x.size(1)),\n",
    "                'lr': 0.003,\n",
    "                'grad_clip': None\n",
    "            },\n",
    "            'gatv2': {\n",
    "                'model': GATv2Net_Matched(data.x.size(1)),\n",
    "                'lr': 0.003,\n",
    "                'grad_clip': None\n",
    "            },\n",
    "            'sage': {\n",
    "                'model': SAGENet_Matched(data.x.size(1)),\n",
    "                'lr': 0.01,\n",
    "                'grad_clip': 2.0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        config = arch_configs[arch.lower()]\n",
    "        model = config['model'].to(device)\n",
    "        lr = config['lr'] if lr is None else lr\n",
    "        grad_clip = config['grad_clip'] if grad_clip is None else grad_clip\n",
    "        \n",
    "        # Class weights for imbalance\n",
    "        y_train = data.y[data.train_mask]\n",
    "        pos = (y_train == 1).sum().float()\n",
    "        neg = (y_train == 0).sum().float()\n",
    "        weight = torch.tensor([1.0, neg/pos.clamp_min(1)], device=device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        # Threshold optimization function\n",
    "        def find_best_threshold(logits, y_true):\n",
    "            probs = F.softmax(logits, dim=1)[:, 1]\n",
    "            best_f1, best_thr = 0, 0.5\n",
    "            \n",
    "            for thr in torch.linspace(0.05, 0.95, 37):\n",
    "                preds = (probs >= thr).long()\n",
    "                y_true_cpu = y_true.cpu().numpy()\n",
    "                preds_cpu = preds.cpu().numpy()\n",
    "                f1 = f1_score(y_true_cpu, preds_cpu, zero_division=0)\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_thr = thr.item()\n",
    "            \n",
    "            return best_thr, best_f1\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_f1 = 0\n",
    "        best_state = None\n",
    "        best_threshold = 0.5\n",
    "        wait = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Train\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            \n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Validate every 5 epochs\n",
    "            if epoch % 5 == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    out = model(data.x, data.edge_index)\n",
    "                    \n",
    "                    if data.val_mask.sum() > 0:\n",
    "                        val_out = out[data.val_mask]\n",
    "                        val_y = data.y[data.val_mask]\n",
    "                        thr, f1 = find_best_threshold(val_out, val_y)\n",
    "                    else:\n",
    "                        # If no validation set, use training for threshold\n",
    "                        train_out = out[data.train_mask]\n",
    "                        train_y = data.y[data.train_mask]\n",
    "                        thr, f1 = find_best_threshold(train_out, train_y)\n",
    "                    \n",
    "                    if f1 > best_val_f1:\n",
    "                        best_val_f1 = f1\n",
    "                        best_threshold = thr\n",
    "                        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "                        wait = 0\n",
    "                        if epoch % 20 == 0:  # Print less frequently\n",
    "                            print(f\"[{arch.upper()}] Epoch {epoch:4d} | Loss: {loss:.4f} | Val F1: {f1:.4f} | Thr: {thr:.2f}\")\n",
    "                    else:\n",
    "                        wait += 5\n",
    "                    \n",
    "                    if wait >= patience:\n",
    "                        print(f\"[{arch.upper()}] Early stopping at epoch {epoch}\")\n",
    "                        break\n",
    "        \n",
    "        # Load best model\n",
    "        if best_state:\n",
    "            model.load_state_dict(best_state)\n",
    "            model.to(device)\n",
    "        \n",
    "        # Extract embeddings for ALL nodes\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass to get embeddings\n",
    "            _ = model(data.x, data.edge_index)\n",
    "            embeddings = model.embeddings.detach().cpu().numpy()\n",
    "            \n",
    "            # Test evaluation\n",
    "            out = model(data.x, data.edge_index)\n",
    "            test_out = out[data.test_mask]\n",
    "            test_y = data.y[data.test_mask]\n",
    "            \n",
    "            probs = F.softmax(test_out, dim=1)[:, 1]\n",
    "            preds = (probs >= best_threshold).long()\n",
    "            \n",
    "            y_true = test_y.cpu().numpy()\n",
    "            y_pred = preds.cpu().numpy()\n",
    "            \n",
    "            test_metrics = {\n",
    "                'F1': f1_score(y_true, y_pred),\n",
    "                'P': precision_score(y_true, y_pred),\n",
    "                'R': recall_score(y_true, y_pred),\n",
    "                'Acc': accuracy_score(y_true, y_pred),\n",
    "                'microF1': f1_score(y_true, y_pred, average='micro')\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'test_metrics': test_metrics,\n",
    "            'val_best_F1': best_val_f1,\n",
    "            'best_threshold': best_threshold,\n",
    "            'embeddings': embeddings,\n",
    "            'embeddings_shape': embeddings.shape\n",
    "        }\n",
    "    \n",
    "    # ============================================\n",
    "    # STEP 5: Train All Models and Save Embeddings\n",
    "    # ============================================\n",
    "    \n",
    "    arch_list = ['gcn', 'skip_gcn', 'gat', 'gatv2', 'sage']\n",
    "    results = {}\n",
    "    embeddings_dict = {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training GNN Models with K-Fold Split\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for arch in arch_list:\n",
    "        print(f\"\\nTraining {arch.upper()}...\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        result = train_gnn_with_embeddings(\n",
    "            data, \n",
    "            arch=arch, \n",
    "            epochs=epochs, \n",
    "            patience=patience\n",
    "        )\n",
    "        \n",
    "        results[arch] = result\n",
    "        embeddings_dict[arch] = result['embeddings']\n",
    "        \n",
    "        # Save embeddings as global variables if requested\n",
    "        if save_embeddings:\n",
    "            var_name = f\"{arch.lower().replace('-', '_')}_embeddings\"\n",
    "            globals()[var_name] = result['embeddings']\n",
    "            print(f\"✓ Saved embeddings to global variable: {var_name} {result['embeddings'].shape}\")\n",
    "    \n",
    "    # ============================================\n",
    "    # STEP 6: Create Results DataFrame\n",
    "    # ============================================\n",
    "    \n",
    "    rows = []\n",
    "    for arch, r in results.items():\n",
    "        m = r['test_metrics']\n",
    "        rows.append({\n",
    "            'Model': arch.upper(),\n",
    "            'F1': m['F1'],\n",
    "            'Precision': m['P'],\n",
    "            'Recall': m['R'],\n",
    "            'Accuracy': m['Acc'],\n",
    "            'Micro-F1': m['microF1'],\n",
    "            'Val best F1': r['val_best_F1'],\n",
    "            'Best thr': r['best_threshold'],\n",
    "            'Embeddings': r['embeddings_shape']\n",
    "        })\n",
    "    \n",
    "    df_results = pd.DataFrame(rows).sort_values('F1', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Format for display\n",
    "    for col in ['F1', 'Precision', 'Recall', 'Accuracy', 'Micro-F1', 'Val best F1']:\n",
    "        if col in df_results.columns:\n",
    "            df_results[col] = df_results[col].round(4)\n",
    "    df_results['Best thr'] = df_results['Best thr'].round(2)\n",
    "    \n",
    "    # ============================================\n",
    "    # STEP 7: Print Final Results\n",
    "    # ============================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL RESULTS - K-Fold GNN Pipeline with Embeddings\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_results.to_string())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Embeddings Saved (Accessible as Global Variables)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for arch in arch_list:\n",
    "        var_name = f\"{arch.lower().replace('-', '_')}_embeddings\"\n",
    "        shape = embeddings_dict[arch].shape\n",
    "        print(f\"  {var_name}: shape {shape}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Configuration Summary\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  K-Fold splits: {n_splits}\")\n",
    "    print(f\"  Current fold: {fold_id}/{n_splits}\")\n",
    "    print(f\"  Validation ratio: {val_ratio}\")\n",
    "    print(f\"  Train-only z-score normalization: ✓\")\n",
    "    print(f\"  Class-weighted loss: ✓\")\n",
    "    print(f\"  Architecture-specific hyperparameters: ✓\")\n",
    "    print(f\"  Graph preprocessing (undirected + self-loops): ✓\")\n",
    "    \n",
    "    print(\"\\nUsage Example:\")\n",
    "    print(\"  # Access embeddings for baseline models\")\n",
    "    print(\"  X_enhanced = np.concatenate([X_original, sage_embeddings], axis=1)\")\n",
    "    print(\"  # Train baseline with enhanced features\")\n",
    "    print(\"  rf_enhanced = RandomForest().fit(X_enhanced[train], y[train])\")\n",
    "    \n",
    "    return df_results, embeddings_dict\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete pipeline\n",
    "    df_results, embeddings = run_elliptic_kfold_with_embeddings(\n",
    "        n_splits=5,\n",
    "        fold_id=1,\n",
    "        val_ratio=0.10,\n",
    "        epochs=1000,\n",
    "        patience=50,\n",
    "        random_state=42,\n",
    "        save_embeddings=True\n",
    "    )\n",
    "    \n",
    "    # After running, you can access embeddings as:\n",
    "    # gcn_embeddings, skip_gcn_embeddings, gat_embeddings, \n",
    "    # gatv2_embeddings, sage_embeddings\n",
    "    \n",
    "    # Example: Enhance baseline model features\n",
    "    # from sklearn.ensemble import RandomForestClassifier\n",
    "    # X_with_sage = np.concatenate([original_features, sage_embeddings], axis=1)\n",
    "    # rf_enhanced = RandomForestClassifier(n_estimators=100)\n",
    "    # rf_enhanced.fit(X_with_sage[train_mask], y[train_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ca8df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BASELINE MODELS ENHANCEMENT WITH GNN EMBEDDINGS (OPTIMIZED)\n",
      "================================================================================\n",
      "\n",
      "1. Loading Elliptic dataset and creating K-fold splits...\n",
      "  Train: 33,525 | Val: 3,726 | Test: 9,313\n",
      "  Features shape: (203769, 165)\n",
      "  Class balance - Illicit: 3,272 / 33,525 (9.8%)\n",
      "  XGBoost scale_pos_weight: 9.25\n",
      "\n",
      "2. Collecting GNN embeddings from global variables...\n",
      "  ✓ Found gcn_embeddings: shape (203769, 64)\n",
      "  ✓ Found skip_gcn_embeddings: shape (203769, 64)\n",
      "  ✓ Found gat_embeddings: shape (203769, 64)\n",
      "  ✓ Found gatv2_embeddings: shape (203769, 64)\n",
      "  ✓ Found sage_embeddings: shape (203769, 64)\n",
      "\n",
      "3. Training baseline models with optimized hyperparameters...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Logistic Regression:\n",
      "  Training baseline (AF only)...\n",
      "    F1: 0.6146 | Precision: 0.4577 | Recall: 0.9351\n",
      "  Training with GCN embeddings (AF+NE)...\n",
      "    F1: 0.8434 | Precision: 0.7734 | Recall: 0.9274\n",
      "  Training with SKIP-GCN embeddings (AF+NE)...\n",
      "    F1: 0.8944 | Precision: 0.8724 | Recall: 0.9175\n",
      "  Training with GAT embeddings (AF+NE)...\n",
      "    F1: 0.7827 | Precision: 0.6719 | Recall: 0.9373\n",
      "  Training with GATV2 embeddings (AF+NE)...\n",
      "    F1: 0.7820 | Precision: 0.6719 | Recall: 0.9351\n",
      "  Training with SAGE embeddings (AF+NE)...\n",
      "    F1: 0.8811 | Precision: 0.8476 | Recall: 0.9175\n",
      "\n",
      "Random Forest:\n",
      "  Training baseline (AF only)...\n",
      "    F1: 0.9290 | Precision: 0.9864 | Recall: 0.8779\n",
      "  Training with GCN embeddings (AF+NE)...\n",
      "    F1: 0.9096 | Precision: 0.9412 | Recall: 0.8801\n",
      "  Training with SKIP-GCN embeddings (AF+NE)...\n",
      "    F1: 0.9310 | Precision: 0.9759 | Recall: 0.8900\n",
      "  Training with GAT embeddings (AF+NE)...\n",
      "    F1: 0.9209 | Precision: 0.9936 | Recall: 0.8581\n",
      "  Training with GATV2 embeddings (AF+NE)...\n",
      "    F1: 0.9209 | Precision: 0.9936 | Recall: 0.8581\n",
      "  Training with SAGE embeddings (AF+NE)...\n",
      "    F1: 0.9212 | Precision: 0.9435 | Recall: 0.8999\n",
      "\n",
      "MLP:\n",
      "  Training baseline (AF only)...\n",
      "    F1: 0.9021 | Precision: 0.9468 | Recall: 0.8614\n",
      "  Training with GCN embeddings (AF+NE)...\n",
      "    F1: 0.8964 | Precision: 0.9181 | Recall: 0.8757\n",
      "  Training with SKIP-GCN embeddings (AF+NE)...\n",
      "    F1: 0.9136 | Precision: 0.9265 | Recall: 0.9010\n",
      "  Training with GAT embeddings (AF+NE)...\n",
      "    F1: 0.8987 | Precision: 0.9042 | Recall: 0.8933\n",
      "  Training with GATV2 embeddings (AF+NE)...\n",
      "    F1: 0.8928 | Precision: 0.9166 | Recall: 0.8702\n",
      "  Training with SAGE embeddings (AF+NE)...\n",
      "    F1: 0.9102 | Precision: 0.9350 | Recall: 0.8867\n",
      "\n",
      "XGBoost:\n",
      "  Training baseline (AF only)...\n",
      "    F1: 0.9271 | Precision: 0.9039 | Recall: 0.9516\n",
      "  Training with GCN embeddings (AF+NE)...\n",
      "    F1: 0.9216 | Precision: 0.9247 | Recall: 0.9186\n",
      "  Training with SKIP-GCN embeddings (AF+NE)...\n",
      "    F1: 0.9281 | Precision: 0.9412 | Recall: 0.9153\n",
      "  Training with GAT embeddings (AF+NE)...\n",
      "    F1: 0.9372 | Precision: 0.9383 | Recall: 0.9362\n",
      "  Training with GATV2 embeddings (AF+NE)...\n",
      "    F1: 0.9217 | Precision: 0.9238 | Recall: 0.9197\n",
      "  Training with SAGE embeddings (AF+NE)...\n",
      "    F1: 0.9197 | Precision: 0.9322 | Recall: 0.9076\n",
      "\n",
      "================================================================================\n",
      "TABLE 1.1 FORMAT - ILLICIT CLASSIFICATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "Logistic Regression:\n",
      "------------------------------------------------------------\n",
      "                              Method  Precision  Illicit Recall     F1\n",
      "              Logistic Regression^AF     0.4577          0.9351 0.6146\n",
      "     Logistic Regression^AF+NE (GCN)     0.7734          0.9274 0.8434\n",
      "Logistic Regression^AF+NE (SKIP-GCN)     0.8724          0.9175 0.8944\n",
      "     Logistic Regression^AF+NE (GAT)     0.6719          0.9373 0.7827\n",
      "   Logistic Regression^AF+NE (GATV2)     0.6719          0.9351 0.7820\n",
      "    Logistic Regression^AF+NE (SAGE)     0.8476          0.9175 0.8811\n",
      "\n",
      "Random Forest:\n",
      "------------------------------------------------------------\n",
      "                        Method  Precision  Illicit Recall     F1\n",
      "              Random Forest^AF     0.9864          0.8779 0.9290\n",
      "     Random Forest^AF+NE (GCN)     0.9412          0.8801 0.9096\n",
      "Random Forest^AF+NE (SKIP-GCN)     0.9759          0.8900 0.9310\n",
      "     Random Forest^AF+NE (GAT)     0.9936          0.8581 0.9209\n",
      "   Random Forest^AF+NE (GATV2)     0.9936          0.8581 0.9209\n",
      "    Random Forest^AF+NE (SAGE)     0.9435          0.8999 0.9212\n",
      "\n",
      "MLP:\n",
      "------------------------------------------------------------\n",
      "              Method  Precision  Illicit Recall     F1\n",
      "              MLP^AF     0.9468          0.8614 0.9021\n",
      "     MLP^AF+NE (GCN)     0.9181          0.8757 0.8964\n",
      "MLP^AF+NE (SKIP-GCN)     0.9265          0.9010 0.9136\n",
      "     MLP^AF+NE (GAT)     0.9042          0.8933 0.8987\n",
      "   MLP^AF+NE (GATV2)     0.9166          0.8702 0.8928\n",
      "    MLP^AF+NE (SAGE)     0.9350          0.8867 0.9102\n",
      "\n",
      "XGBoost:\n",
      "------------------------------------------------------------\n",
      "                  Method  Precision  Illicit Recall     F1\n",
      "              XGBoost^AF     0.9039          0.9516 0.9271\n",
      "     XGBoost^AF+NE (GCN)     0.9247          0.9186 0.9216\n",
      "XGBoost^AF+NE (SKIP-GCN)     0.9412          0.9153 0.9281\n",
      "     XGBoost^AF+NE (GAT)     0.9383          0.9362 0.9372\n",
      "   XGBoost^AF+NE (GATV2)     0.9238          0.9197 0.9217\n",
      "    XGBoost^AF+NE (SAGE)     0.9322          0.9076 0.9197\n",
      "\n",
      "================================================================================\n",
      "SUMMARY - ENHANCEMENT FROM GNN EMBEDDINGS\n",
      "================================================================================\n",
      "Logistic Regression  | Baseline: 0.6146 → Best: 0.8944 (SKIP-GCN) | +45.5%\n",
      "Random Forest        | Baseline: 0.9290 → Best: 0.9310 (SKIP-GCN) | +0.2%\n",
      "MLP                  | Baseline: 0.9021 → Best: 0.9136 (SKIP-GCN) | +1.3%\n",
      "XGBoost              | Baseline: 0.9271 → Best: 0.9372 (GAT) | +1.1%\n",
      "\n",
      "================================================================================\n",
      "COMPLETE RESULTS TABLE\n",
      "================================================================================\n",
      "                              Method  Precision  Illicit Recall     F1\n",
      "              Logistic Regression^AF     0.4577          0.9351 0.6146\n",
      "     Logistic Regression^AF+NE (GCN)     0.7734          0.9274 0.8434\n",
      "Logistic Regression^AF+NE (SKIP-GCN)     0.8724          0.9175 0.8944\n",
      "     Logistic Regression^AF+NE (GAT)     0.6719          0.9373 0.7827\n",
      "   Logistic Regression^AF+NE (GATV2)     0.6719          0.9351 0.7820\n",
      "    Logistic Regression^AF+NE (SAGE)     0.8476          0.9175 0.8811\n",
      "                    Random Forest^AF     0.9864          0.8779 0.9290\n",
      "           Random Forest^AF+NE (GCN)     0.9412          0.8801 0.9096\n",
      "      Random Forest^AF+NE (SKIP-GCN)     0.9759          0.8900 0.9310\n",
      "           Random Forest^AF+NE (GAT)     0.9936          0.8581 0.9209\n",
      "         Random Forest^AF+NE (GATV2)     0.9936          0.8581 0.9209\n",
      "          Random Forest^AF+NE (SAGE)     0.9435          0.8999 0.9212\n",
      "                              MLP^AF     0.9468          0.8614 0.9021\n",
      "                     MLP^AF+NE (GCN)     0.9181          0.8757 0.8964\n",
      "                MLP^AF+NE (SKIP-GCN)     0.9265          0.9010 0.9136\n",
      "                     MLP^AF+NE (GAT)     0.9042          0.8933 0.8987\n",
      "                   MLP^AF+NE (GATV2)     0.9166          0.8702 0.8928\n",
      "                    MLP^AF+NE (SAGE)     0.9350          0.8867 0.9102\n",
      "                          XGBoost^AF     0.9039          0.9516 0.9271\n",
      "                 XGBoost^AF+NE (GCN)     0.9247          0.9186 0.9216\n",
      "            XGBoost^AF+NE (SKIP-GCN)     0.9412          0.9153 0.9281\n",
      "                 XGBoost^AF+NE (GAT)     0.9383          0.9362 0.9372\n",
      "               XGBoost^AF+NE (GATV2)     0.9238          0.9197 0.9217\n",
      "                XGBoost^AF+NE (SAGE)     0.9322          0.9076 0.9197\n",
      "\n",
      "✓ Results saved to 'baseline_enhancement_results_optimized.csv'\n",
      "\n",
      "================================================================================\n",
      "Expected Performance (based on your K-fold baseline results):\n",
      "  Random Forest: ~0.93-0.94 F1 baseline\n",
      "  XGBoost: ~0.92-0.93 F1 baseline\n",
      "  MLP: ~0.89-0.90 F1 baseline\n",
      "  Logistic Regression: ~0.61-0.62 F1 baseline\n",
      "\n",
      "With GNN embeddings, expect 5-70% relative improvement depending on model.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Baseline Models Enhancement with GNN Embeddings (K-Fold) - Optimized\n",
    "=====================================================================\n",
    "Updated with hyperparameters matching your high-performing baseline configuration.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def train_baselines_with_embeddings_optimized(\n",
    "    n_splits=5,\n",
    "    fold_id=1,\n",
    "    val_ratio=0.10,\n",
    "    random_state=42,\n",
    "    save_detailed_results=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Train baseline models with and without GNN embeddings using optimized hyperparameters.\n",
    "    \n",
    "    This function uses the exact hyperparameters from your high-performing baseline models:\n",
    "    - RF: max_depth=20, n_estimators=100\n",
    "    - MLP: (100, 50) hidden layers with StandardScaler\n",
    "    - XGBoost: Dynamic scale_pos_weight calculation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_splits : int\n",
    "        Number of K-fold splits (should match GNN training)\n",
    "    fold_id : int\n",
    "        Which fold to use (should match GNN training)\n",
    "    val_ratio : float\n",
    "        Validation split ratio (should match GNN training)\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    save_detailed_results : bool\n",
    "        Whether to save detailed results to CSV\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df_results : DataFrame\n",
    "        Comprehensive results for all models and configurations\n",
    "    detailed_results : dict\n",
    "        Detailed results for each model/configuration\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"BASELINE MODELS ENHANCEMENT WITH GNN EMBEDDINGS (OPTIMIZED)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ============================================\n",
    "    # STEP 1: Load Data and Create Same K-Fold Split\n",
    "    # ============================================\n",
    "    \n",
    "    print(\"\\n1. Loading Elliptic dataset and creating K-fold splits...\")\n",
    "    \n",
    "    # Load raw data\n",
    "    features_path = 'elliptic_txs_features.csv'\n",
    "    classes_path = 'elliptic_txs_classes.csv'\n",
    "    \n",
    "    # Load features\n",
    "    df_features = pd.read_csv(features_path, header=None)\n",
    "    col_names = ['tx_id', 'time_step']\n",
    "    col_names += [f'local_{i}' for i in range(1, 94)]\n",
    "    col_names += [f'aggregated_{i}' for i in range(1, 73)]\n",
    "    df_features.columns = col_names\n",
    "    \n",
    "    df_features['tx_id'] = df_features['tx_id'].astype(int)\n",
    "    df_features['time_step'] = df_features['time_step'].astype(int)\n",
    "    \n",
    "    # Load classes\n",
    "    df_classes = pd.read_csv(classes_path)\n",
    "    df_features = df_features.merge(\n",
    "        df_classes.rename(columns={'txId': 'tx_id'}),\n",
    "        on='tx_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Extract features and labels\n",
    "    feature_cols = [col for col in df_features.columns \n",
    "                   if col.startswith(('local_', 'aggregated_'))]\n",
    "    X = df_features[feature_cols].values\n",
    "    \n",
    "    # Create labels: 0=licit, 1=illicit, -1=unknown\n",
    "    y = np.full(len(df_features), -1, dtype=int)\n",
    "    class_values = df_features['class'].astype(str).values\n",
    "    y[class_values == '2'] = 0  # licit\n",
    "    y[class_values == '1'] = 1  # illicit\n",
    "    \n",
    "    # Create same K-fold split as GNN training\n",
    "    labeled_indices = np.where(y != -1)[0]\n",
    "    y_labeled = y[labeled_indices]\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    splits = list(skf.split(np.zeros_like(y_labeled), y_labeled))\n",
    "    \n",
    "    train_fold_idx, test_fold_idx = splits[fold_id - 1]\n",
    "    train_labeled = labeled_indices[train_fold_idx]\n",
    "    test_labeled = labeled_indices[test_fold_idx]\n",
    "    \n",
    "    # Create validation split if needed\n",
    "    if val_ratio > 0:\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=random_state)\n",
    "        y_train_labeled = y[train_labeled]\n",
    "        train_keep_idx, val_idx = next(sss.split(np.zeros_like(y_train_labeled), y_train_labeled))\n",
    "        \n",
    "        final_train = train_labeled[train_keep_idx]\n",
    "        val_indices = train_labeled[val_idx]\n",
    "    else:\n",
    "        final_train = train_labeled\n",
    "        val_indices = np.array([], dtype=int)\n",
    "    \n",
    "    # Create masks\n",
    "    N = len(df_features)\n",
    "    train_mask = np.zeros(N, dtype=bool)\n",
    "    val_mask = np.zeros(N, dtype=bool)\n",
    "    test_mask = np.zeros(N, dtype=bool)\n",
    "    \n",
    "    train_mask[final_train] = True\n",
    "    if val_indices.size > 0:\n",
    "        val_mask[val_indices] = True\n",
    "    test_mask[test_labeled] = True\n",
    "    \n",
    "    # Calculate class balance for XGBoost\n",
    "    y_train = y[train_mask]\n",
    "    pos_count = (y_train == 1).sum()\n",
    "    neg_count = (y_train == 0).sum()\n",
    "    scale_pos_weight = float(neg_count) / float(max(pos_count, 1))\n",
    "    \n",
    "    print(f\"  Train: {train_mask.sum():,} | Val: {val_mask.sum():,} | Test: {test_mask.sum():,}\")\n",
    "    print(f\"  Features shape: {X.shape}\")\n",
    "    print(f\"  Class balance - Illicit: {pos_count:,} / {pos_count+neg_count:,} ({pos_count/(pos_count+neg_count)*100:.1f}%)\")\n",
    "    print(f\"  XGBoost scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "    \n",
    "    # ============================================\n",
    "    # STEP 2: Collect GNN Embeddings\n",
    "    # ============================================\n",
    "    \n",
    "    print(\"\\n2. Collecting GNN embeddings from global variables...\")\n",
    "    \n",
    "    embeddings_dict = {}\n",
    "    gnn_models = ['gcn', 'skip_gcn', 'gat', 'gatv2', 'sage']\n",
    "    \n",
    "    for model_name in gnn_models:\n",
    "        var_name = f\"{model_name}_embeddings\"\n",
    "        if var_name in globals():\n",
    "            embeddings_dict[model_name] = globals()[var_name]\n",
    "            print(f\"  ✓ Found {var_name}: shape {embeddings_dict[model_name].shape}\")\n",
    "        else:\n",
    "            print(f\"  ✗ Warning: {var_name} not found in global variables\")\n",
    "    \n",
    "    if not embeddings_dict:\n",
    "        raise ValueError(\"No GNN embeddings found! Please run GNN training first.\")\n",
    "    \n",
    "    # ============================================\n",
    "    # STEP 3: Define Optimized Baseline Models\n",
    "    # ============================================\n",
    "    \n",
    "    def get_baseline_models():\n",
    "        \"\"\"Get baseline model configurations with optimized hyperparameters.\"\"\"\n",
    "        return {\n",
    "            'Logistic Regression': {\n",
    "                'model': LogisticRegression(\n",
    "                    max_iter=1000,\n",
    "                    class_weight='balanced',\n",
    "                    random_state=random_state\n",
    "                ),\n",
    "                'needs_scaling': False\n",
    "            },\n",
    "            'Random Forest': {\n",
    "                'model': RandomForestClassifier(\n",
    "                    n_estimators=100,\n",
    "                    max_depth=20,  # Updated from 10 to 20\n",
    "                    class_weight='balanced',\n",
    "                    random_state=random_state,\n",
    "                    n_jobs=-1\n",
    "                ),\n",
    "                'needs_scaling': False\n",
    "            },\n",
    "            'MLP': {\n",
    "                'model': MLPClassifier(\n",
    "                    hidden_layer_sizes=(100, 50),  # Updated from (128, 64)\n",
    "                    activation='relu',\n",
    "                    solver='adam',\n",
    "                    alpha=0.001,\n",
    "                    max_iter=500,\n",
    "                    early_stopping=True,\n",
    "                    validation_fraction=0.1,\n",
    "                    random_state=random_state\n",
    "                ),\n",
    "                'needs_scaling': True  # MLP needs StandardScaler\n",
    "            },\n",
    "            'XGBoost': {\n",
    "                'model': XGBClassifier(\n",
    "                    n_estimators=100,\n",
    "                    max_depth=6,\n",
    "                    learning_rate=0.1,\n",
    "                    scale_pos_weight=scale_pos_weight,  # Dynamic calculation\n",
    "                    use_label_encoder=False,\n",
    "                    eval_metric='logloss',\n",
    "                    random_state=random_state\n",
    "                ),\n",
    "                'needs_scaling': False\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # ============================================\n",
    "    # STEP 4: Training Function\n",
    "    # ============================================\n",
    "    \n",
    "    def train_and_evaluate(X_train, y_train, X_test, y_test, model_config):\n",
    "        \"\"\"Train model and return metrics with optional scaling.\"\"\"\n",
    "        model = model_config['model']\n",
    "        needs_scaling = model_config['needs_scaling']\n",
    "        \n",
    "        if needs_scaling:\n",
    "            # Scale data for models that need it (MLP)\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "            'Illicit Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "            'F1': f1_score(y_test, y_pred, zero_division=0),\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'MicroF1': f1_score(y_test, y_pred, average='micro')\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    # ============================================\n",
    "    # STEP 5: Train All Configurations\n",
    "    # ============================================\n",
    "    \n",
    "    print(\"\\n3. Training baseline models with optimized hyperparameters...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    all_results = []\n",
    "    detailed_results = {}\n",
    "    \n",
    "    # For each baseline model\n",
    "    for model_name, model_config in get_baseline_models().items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        \n",
    "        # 1. Train baseline (features only - AF)\n",
    "        print(f\"  Training baseline (AF only)...\")\n",
    "        X_train = X[train_mask]\n",
    "        y_train = y[train_mask]\n",
    "        X_test = X[test_mask]\n",
    "        y_test = y[test_mask]\n",
    "        \n",
    "        metrics = train_and_evaluate(X_train, y_train, X_test, y_test, model_config)\n",
    "        \n",
    "        result_row = {\n",
    "            'Method': f\"{model_name}^AF\",\n",
    "            **metrics\n",
    "        }\n",
    "        all_results.append(result_row)\n",
    "        detailed_results[f\"{model_name}_AF\"] = metrics\n",
    "        \n",
    "        print(f\"    F1: {metrics['F1']:.4f} | Precision: {metrics['Precision']:.4f} | Recall: {metrics['Illicit Recall']:.4f}\")\n",
    "        \n",
    "        # 2. Train with each GNN's embeddings (AF+NE)\n",
    "        for gnn_name, embeddings in embeddings_dict.items():\n",
    "            gnn_display = gnn_name.upper().replace('_', '-')\n",
    "            print(f\"  Training with {gnn_display} embeddings (AF+NE)...\")\n",
    "            \n",
    "            # Concatenate features with embeddings\n",
    "            X_enhanced = np.concatenate([X, embeddings], axis=1)\n",
    "            \n",
    "            X_train_enh = X_enhanced[train_mask]\n",
    "            X_test_enh = X_enhanced[test_mask]\n",
    "            \n",
    "            # Get fresh model instance\n",
    "            model_config_fresh = get_baseline_models()[model_name]\n",
    "            \n",
    "            metrics = train_and_evaluate(X_train_enh, y_train, X_test_enh, y_test, \n",
    "                                        model_config_fresh)\n",
    "            \n",
    "            result_row = {\n",
    "                'Method': f\"{model_name}^AF+NE ({gnn_display})\",\n",
    "                **metrics\n",
    "            }\n",
    "            all_results.append(result_row)\n",
    "            detailed_results[f\"{model_name}_AF+NE_{gnn_name}\"] = metrics\n",
    "            \n",
    "            print(f\"    F1: {metrics['F1']:.4f} | Precision: {metrics['Precision']:.4f} | Recall: {metrics['Illicit Recall']:.4f}\")\n",
    "    \n",
    "    # ============================================\n",
    "    # STEP 6: Create Results DataFrame\n",
    "    # ============================================\n",
    "    \n",
    "    df_results = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Round numeric columns\n",
    "    numeric_cols = ['Precision', 'Illicit Recall', 'F1', 'Accuracy', 'MicroF1']\n",
    "    for col in numeric_cols:\n",
    "        df_results[col] = df_results[col].round(4)\n",
    "    \n",
    "    # ============================================\n",
    "    # STEP 7: Display Results in Paper Format\n",
    "    # ============================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TABLE 1.1 FORMAT - ILLICIT CLASSIFICATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display columns matching the paper\n",
    "    display_cols = ['Method', 'Precision', 'Illicit Recall', 'F1']\n",
    "    df_display = df_results[display_cols].copy()\n",
    "    \n",
    "    print(\"\\nLogistic Regression:\")\n",
    "    print(\"-\" * 60)\n",
    "    lr_results = df_display[df_display['Method'].str.startswith('Logistic')]\n",
    "    print(lr_results.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nRandom Forest:\")\n",
    "    print(\"-\" * 60)\n",
    "    rf_results = df_display[df_display['Method'].str.startswith('Random')]\n",
    "    print(rf_results.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nMLP:\")\n",
    "    print(\"-\" * 60)\n",
    "    mlp_results = df_display[df_display['Method'].str.startswith('MLP')]\n",
    "    print(mlp_results.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nXGBoost:\")\n",
    "    print(\"-\" * 60)\n",
    "    xgb_results = df_display[df_display['Method'].str.startswith('XGBoost')]\n",
    "    print(xgb_results.to_string(index=False))\n",
    "    \n",
    "    # ============================================\n",
    "    # STEP 8: Summary of Improvements\n",
    "    # ============================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY - ENHANCEMENT FROM GNN EMBEDDINGS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    baseline_models = ['Logistic Regression', 'Random Forest', 'MLP', 'XGBoost']\n",
    "    \n",
    "    for baseline in baseline_models:\n",
    "        # Get baseline F1\n",
    "        baseline_f1 = df_results[df_results['Method'] == f\"{baseline}^AF\"]['F1'].values[0]\n",
    "        \n",
    "        # Get best enhanced F1\n",
    "        enhanced_results = df_results[df_results['Method'].str.startswith(f\"{baseline}^AF+NE\")]\n",
    "        if not enhanced_results.empty:\n",
    "            best_idx = enhanced_results['F1'].idxmax()\n",
    "            best_f1 = enhanced_results.loc[best_idx, 'F1']\n",
    "            best_method = enhanced_results.loc[best_idx, 'Method']\n",
    "            best_gnn = best_method.split('(')[1].replace(')', '')\n",
    "            \n",
    "            improvement = ((best_f1 - baseline_f1) / baseline_f1) * 100\n",
    "            print(f\"{baseline:20} | Baseline: {baseline_f1:.4f} → Best: {best_f1:.4f} ({best_gnn}) | +{improvement:.1f}%\")\n",
    "    \n",
    "    # ============================================\n",
    "    # STEP 9: Complete Table (All Results)\n",
    "    # ============================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLETE RESULTS TABLE\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_display.to_string(index=False))\n",
    "    \n",
    "    if save_detailed_results:\n",
    "        df_results.to_csv('baseline_enhancement_results_optimized.csv', index=False)\n",
    "        print(\"\\n✓ Results saved to 'baseline_enhancement_results_optimized.csv'\")\n",
    "    \n",
    "    return df_results, detailed_results\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# USAGE\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Assumes GNN embeddings are already available as global variables\n",
    "    # from: run_elliptic_kfold_with_embeddings()\n",
    "    \n",
    "    # Train baseline models with optimized hyperparameters\n",
    "    df_results, detailed_results = train_baselines_with_embeddings_optimized(\n",
    "        n_splits=5,\n",
    "        fold_id=1,  # Use same fold as GNN training\n",
    "        val_ratio=0.10,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Expected Performance (based on your K-fold baseline results):\")\n",
    "    print(\"  Random Forest: ~0.93-0.94 F1 baseline\")\n",
    "    print(\"  XGBoost: ~0.92-0.93 F1 baseline\")\n",
    "    print(\"  MLP: ~0.89-0.90 F1 baseline\")\n",
    "    print(\"  Logistic Regression: ~0.61-0.62 F1 baseline\")\n",
    "    print(\"\\nWith GNN embeddings, expect 5-70% relative improvement depending on model.\")\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
